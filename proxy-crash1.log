Sep 23 20:12:28 k8stian-m1 kube-apiserver: I0923 20:12:28.762381    1528 controller.go:107] OpenAPI AggregationController: Processing item v1beta1.metrics.k8s.io
Sep 23 20:12:28 k8stian-m1 kube-apiserver: E0923 20:12:28.766466    1528 controller.go:114] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: OpenAPI spec does not exist
Sep 23 20:12:28 k8stian-m1 kube-apiserver: I0923 20:12:28.766479    1528 controller.go:127] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.217233    1530 rest_metrics_client.go:84] missing resource metric cpu for container istio-proxy in pod istio-system/istio-policy-5f9cf6df57-6b987
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.218536    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-policy", UID:"7658e00b-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608125", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' did not receive metrics for any ready pods
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.225100    1530 horizontal.go:811] Successfully updated status for istio-policy
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: E0923 20:12:29.225129    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-policy: failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.225559    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-policy", UID:"7658e00b-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608125", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.236758    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-telemetry", UID:"765b1b02-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608126", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.243816    1530 horizontal.go:811] Successfully updated status for istio-telemetry
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: E0923 20:12:29.243853    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-telemetry: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.244533    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-telemetry", UID:"765b1b02-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608126", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.257136    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"76554b4d-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608127", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.273811    1530 horizontal.go:811] Successfully updated status for istio-ingressgateway
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: E0923 20:12:29.273842    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.274606    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"76554b4d-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608127", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.287439    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-pilot", UID:"765cca23-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608128", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.295889    1530 horizontal.go:811] Successfully updated status for istio-pilot
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: E0923 20:12:29.295944    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-pilot: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:29 k8stian-m1 kube-controller-manager: I0923 20:12:29.295976    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-pilot", UID:"765cca23-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608128", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:12:39 k8stian-m1 kube-controller-manager: I0923 20:12:39.368553    1530 horizontal.go:811] Successfully updated status for istio-policy
Sep 23 20:12:39 k8stian-m1 kube-controller-manager: I0923 20:12:39.386246    1530 horizontal.go:811] Successfully updated status for istio-telemetry
Sep 23 20:12:39 k8stian-m1 kube-controller-manager: I0923 20:12:39.402513    1530 horizontal.go:811] Successfully updated status for istio-ingressgateway
Sep 23 20:12:39 k8stian-m1 kube-controller-manager: I0923 20:12:39.418280    1530 horizontal.go:811] Successfully updated status for istio-pilot
Sep 23 20:12:49 k8stian-m1 kube-controller-manager: I0923 20:12:49.665813    1530 horizontal.go:811] Successfully updated status for istio-telemetry
Sep 23 20:12:59 k8stian-m1 kube-controller-manager: I0923 20:12:59.869220    1530 rest_metrics_client.go:84] missing resource metric cpu for container istio-proxy in pod istio-system/istio-policy-5f9cf6df57-6b987
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.871408    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-policy", UID:"7658e00b-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608210", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' did not receive metrics for any ready pods
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.880618    1530 horizontal.go:811] Successfully updated status for istio-policy
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: E0923 20:12:59.880656    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-policy: failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.880932    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-policy", UID:"7658e00b-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608210", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.899658    1530 horizontal.go:811] Successfully updated status for istio-telemetry
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.909148    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"76554b4d-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608212", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.923188    1530 horizontal.go:811] Successfully updated status for istio-ingressgateway
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: E0923 20:12:59.923250    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.923758    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"76554b4d-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608212", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.932148    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-pilot", UID:"765cca23-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608213", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.939379    1530 horizontal.go:811] Successfully updated status for istio-pilot
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: E0923 20:12:59.939421    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-pilot: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:00 k8stian-m1 kube-controller-manager: I0923 20:12:59.939497    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-pilot", UID:"765cca23-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608213", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:01 k8stian-m1 kubelet: W0923 20:13:01.046734    1621 reflector.go:289] object-"kube-system"/"fluentd-es-config-v0.2.0": watch of *v1.ConfigMap ended with: too old resource version: 15603199 (15603924)
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: I0923 20:13:10.390635    1530 horizontal.go:811] Successfully updated status for istio-policy
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: I0923 20:13:10.408350    1530 rest_metrics_client.go:84] missing resource metric cpu for container mixer in pod istio-system/istio-telemetry-7749c6d54f-jlvpn
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: I0923 20:13:10.408577    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-telemetry", UID:"765b1b02-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608266", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' did not receive metrics for any ready pods
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: I0923 20:13:10.419022    1530 horizontal.go:811] Successfully updated status for istio-telemetry
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: E0923 20:13:10.419102    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-telemetry: failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: I0923 20:13:10.419272    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-telemetry", UID:"765b1b02-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608266", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: I0923 20:13:10.436836    1530 horizontal.go:811] Successfully updated status for istio-ingressgateway
Sep 23 20:13:10 k8stian-m1 kube-controller-manager: I0923 20:13:10.450021    1530 horizontal.go:811] Successfully updated status for istio-pilot
Sep 23 20:13:20 k8stian-m1 kube-controller-manager: I0923 20:13:20.772693    1530 horizontal.go:811] Successfully updated status for istio-telemetry
Sep 23 20:13:20 k8stian-m1 kube-controller-manager: E0923 20:13:20.785839    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-telemetry: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:20 k8stian-m1 kube-controller-manager: I0923 20:13:20.749579    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-telemetry", UID:"765b1b02-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608294", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:20 k8stian-m1 kube-controller-manager: I0923 20:13:20.786840    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-telemetry", UID:"765b1b02-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608294", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:28 k8stian-m1 kube-apiserver: I0923 20:13:28.797745    1528 controller.go:107] OpenAPI AggregationController: Processing item v1beta1.metrics.k8s.io
Sep 23 20:13:29 k8stian-m1 kube-apiserver: E0923 20:13:28.835292    1528 controller.go:114] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: OpenAPI spec does not exist
Sep 23 20:13:29 k8stian-m1 kube-apiserver: I0923 20:13:28.835311    1528 controller.go:127] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:30.992504    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-policy", UID:"7658e00b-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608293", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.001123    1530 horizontal.go:811] Successfully updated status for istio-policy
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.001299    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-policy", UID:"7658e00b-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608293", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: E0923 20:13:31.001581    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-policy: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.022380    1530 horizontal.go:811] Successfully updated status for istio-telemetry
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.032544    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"76554b4d-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608295", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.038089    1530 horizontal.go:811] Successfully updated status for istio-ingressgateway
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: E0923 20:13:31.038111    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.038557    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"76554b4d-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608295", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.046369    1530 rest_metrics_client.go:84] missing resource metric cpu for container discovery in pod istio-system/istio-pilot-5544b58bb6-kt4st
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.046913    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-pilot", UID:"765cca23-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608296", FieldPath:""}): type: 'Warning' reason: 'FailedGetResourceMetric' did not receive metrics for any ready pods
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.052889    1530 horizontal.go:811] Successfully updated status for istio-pilot
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: E0923 20:13:31.052912    1530 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-pilot: failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:13:31 k8stian-m1 kube-controller-manager: I0923 20:13:31.053080    1530 event.go:209] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-pilot", UID:"765cca23-ae25-11e9-926a-52540084153b", APIVersion:"autoscaling/v2beta2", ResourceVersion:"15608296", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' failed to get cpu utilization: did not receive metrics for any ready pods
Sep 23 20:13:43 k8stian-m1 kube-proxy: I0923 20:13:42.149722    1623 trace.go:81] Trace[1645947214]: "iptables save" (started: 2019-09-23 20:13:39.089221784 +0800 CST m=+4152404.899085096) (total time: 2.418646254s):
Sep 23 20:13:47 k8stian-m1 kube-proxy: Trace[1645947214]: [2.418646254s] [2.418646254s] END
Sep 23 20:16:49 k8stian-m1 journal: Permanent journal is using 2.0G (max allowed 2.0G, trying to leave 2.5G free of 8.9G available â†’ current limit 2.0G).
Sep 23 20:16:49 k8stian-m1 journal: Journal started
Sep 23 20:16:49 k8stian-m1 systemd: kube-scheduler.service: main process exited, code=exited, status=1/FAILURE
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.707139    6755 flags.go:33] FLAG: --address="0.0.0.0"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798806    6755 flags.go:33] FLAG: --algorithm-provider=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798812    6755 flags.go:33] FLAG: --alsologtostderr="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798820    6755 flags.go:33] FLAG: --authentication-kubeconfig="/etc/kubernetes/kube-scheduler.kubeconfig"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798825    6755 flags.go:33] FLAG: --authentication-skip-lookup="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798831    6755 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl="10s"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798839    6755 flags.go:33] FLAG: --authentication-tolerate-lookup-failure="true"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798842    6755 flags.go:33] FLAG: --authorization-always-allow-paths="[/healthz]"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798877    6755 flags.go:33] FLAG: --authorization-kubeconfig="/etc/kubernetes/kube-scheduler.kubeconfig"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798881    6755 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl="10s"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798897    6755 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl="10s"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798902    6755 flags.go:33] FLAG: --bind-address="10.11.37.71"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798911    6755 flags.go:33] FLAG: --cert-dir=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798915    6755 flags.go:33] FLAG: --client-ca-file="/etc/kubernetes/cert/ca.pem"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798919    6755 flags.go:33] FLAG: --config="/etc/kubernetes/kube-scheduler.yaml"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798925    6755 flags.go:33] FLAG: --contention-profiling="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798929    6755 flags.go:33] FLAG: --failure-domains="kubernetes.io/hostname,failure-domain.beta.kubernetes.io/zone,failure-domain.beta.kubernetes.io/region"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798939    6755 flags.go:33] FLAG: --feature-gates=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798949    6755 flags.go:33] FLAG: --hard-pod-affinity-symmetric-weight="1"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798955    6755 flags.go:33] FLAG: --help="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798959    6755 flags.go:33] FLAG: --http2-max-streams-per-connection="0"
Sep 23 20:16:49 k8stian-m1 systemd: Unit kube-scheduler.service entered failed state.
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798964    6755 flags.go:33] FLAG: --kube-api-burst="100"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798969    6755 flags.go:33] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798973    6755 flags.go:33] FLAG: --kube-api-qps="50"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798983    6755 flags.go:33] FLAG: --kubeconfig=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798987    6755 flags.go:33] FLAG: --leader-elect="true"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798991    6755 flags.go:33] FLAG: --leader-elect-lease-duration="15s"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798995    6755 flags.go:33] FLAG: --leader-elect-renew-deadline="10s"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.798999    6755 flags.go:33] FLAG: --leader-elect-resource-lock="endpoints"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799002    6755 flags.go:33] FLAG: --leader-elect-retry-period="2s"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799006    6755 flags.go:33] FLAG: --lock-object-name="kube-scheduler"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799009    6755 flags.go:33] FLAG: --lock-object-namespace="kube-system"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799014    6755 flags.go:33] FLAG: --log-backtrace-at=":0"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799023    6755 flags.go:33] FLAG: --log-dir=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799028    6755 flags.go:33] FLAG: --log-file=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799031    6755 flags.go:33] FLAG: --log-flush-frequency="5s"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799035    6755 flags.go:33] FLAG: --logtostderr="true"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799038    6755 flags.go:33] FLAG: --master=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799042    6755 flags.go:33] FLAG: --policy-config-file=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799045    6755 flags.go:33] FLAG: --policy-configmap=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799049    6755 flags.go:33] FLAG: --policy-configmap-namespace="kube-system"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799053    6755 flags.go:33] FLAG: --port="0"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799057    6755 flags.go:33] FLAG: --profiling="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799061    6755 flags.go:33] FLAG: --requestheader-allowed-names="[]"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799066    6755 flags.go:33] FLAG: --requestheader-client-ca-file="/etc/kubernetes/cert/ca.pem"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799071    6755 flags.go:33] FLAG: --requestheader-extra-headers-prefix="[X-Remote-Extra-]"
Sep 23 20:16:49 k8stian-m1 systemd: kube-scheduler.service failed.
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799089    6755 flags.go:33] FLAG: --requestheader-group-headers="[X-Remote-Group]"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799094    6755 flags.go:33] FLAG: --requestheader-username-headers="[X-Remote-User]"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799101    6755 flags.go:33] FLAG: --scheduler-name="default-scheduler"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799105    6755 flags.go:33] FLAG: --secure-port="10259"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799109    6755 flags.go:33] FLAG: --skip-headers="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799113    6755 flags.go:33] FLAG: --stderrthreshold="2"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799116    6755 flags.go:33] FLAG: --tls-cert-file="/etc/kubernetes/cert/kube-scheduler.pem"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799120    6755 flags.go:33] FLAG: --tls-cipher-suites="[]"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799125    6755 flags.go:33] FLAG: --tls-min-version=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799128    6755 flags.go:33] FLAG: --tls-private-key-file="/etc/kubernetes/cert/kube-scheduler-key.pem"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799133    6755 flags.go:33] FLAG: --tls-sni-cert-key="[]"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799139    6755 flags.go:33] FLAG: --use-legacy-policy-config="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799145    6755 flags.go:33] FLAG: --v="2"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799149    6755 flags.go:33] FLAG: --version="false"
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799172    6755 flags.go:33] FLAG: --vmodule=""
Sep 23 20:16:49 k8stian-m1 kube-scheduler: I0923 20:16:49.799176    6755 flags.go:33] FLAG: --write-config-to=""
Sep 23 20:16:49 k8stian-m1 systemd: kube-scheduler.service holdoff time over, scheduling restart.
Sep 23 20:16:49 k8stian-m1 systemd: Stopped Kubernetes Scheduler.
Sep 23 20:16:49 k8stian-m1 systemd: Started Kubernetes Scheduler.
Sep 23 20:16:49 k8stian-m1 systemd: systemd-journald.service watchdog timeout (limit 3min)!
Sep 23 20:16:49 k8stian-m1 dockerd: http: multiple response.WriteHeader calls
Sep 23 20:16:49 k8stian-m1 dockerd: http: multiple response.WriteHeader calls
Sep 23 20:16:49 k8stian-m1 systemd: Starting Flush Journal to Persistent Storage...
Sep 23 20:16:49 k8stian-m1 rsyslogd: sd_journal_get_cursor() failed: 'Cannot assign requested address'  [v8.24.0-34.el7]
Sep 23 20:16:50 k8stian-m1 systemd: kube-controller-manager.service: main process exited, code=exited, status=255/n/a
Sep 23 20:16:50 k8stian-m1 systemd: Unit kube-controller-manager.service entered failed state.
Sep 23 20:16:50 k8stian-m1 systemd: kube-controller-manager.service failed.
Sep 23 20:16:50 k8stian-m1 systemd: Started Flush Journal to Persistent Storage.
Sep 23 20:16:50 k8stian-m1 kube-scheduler: I0923 20:16:50.684336    6755 server.go:142] Version: v1.14.5
Sep 23 20:16:50 k8stian-m1 kube-scheduler: I0923 20:16:50.684388    6755 defaults.go:87] TaintNodesByCondition is enabled, PodToleratesNodeTaints predicate is mandatory
Sep 23 20:16:50 k8stian-m1 kube-scheduler: I0923 20:16:50.684400    6755 server.go:161] Starting Kubernetes Scheduler version v1.14.5
Sep 23 20:16:50 k8stian-m1 kube-scheduler: I0923 20:16:50.699730    6755 factory.go:331] Creating scheduler from algorithm provider 'DefaultProvider'
Sep 23 20:16:50 k8stian-m1 kube-scheduler: I0923 20:16:50.699769    6755 factory.go:412] Creating scheduler with fit predicates 'map[CheckNodeUnschedulable:{} CheckVolumeBinding:{} GeneralPredicates:{} MatchInterPodAffinity:{} MaxAzureDiskVolumeCount:{} MaxCSIVolumeCountPred:{} MaxEBSVolumeCount:{} MaxGCEPDVolumeCount:{} NoDiskConflict:{} NoVolumeZoneConflict:{} PodToleratesNodeTaints:{}]' and priority functions 'map[BalancedResourceAllocation:{} ImageLocalityPriority:{} InterPodAffinityPriority:{} LeastRequestedPriority:{} NodeAffinityPriority:{} NodePreferAvoidPodsPriority:{} SelectorSpreadPriority:{} TaintTolerationPriority:{}]'
Sep 23 20:16:50 k8stian-m1 kube-scheduler: W0923 20:16:50.708704    6755 authorization.go:47] Authorization is disabled
Sep 23 20:16:50 k8stian-m1 kube-scheduler: W0923 20:16:50.708751    6755 authentication.go:55] Authentication is disabled
Sep 23 20:16:50 k8stian-m1 kube-scheduler: I0923 20:16:50.708769    6755 deprecated_insecure_serving.go:49] Serving healthz insecurely on 10.11.37.71:10251
Sep 23 20:16:50 k8stian-m1 kube-scheduler: I0923 20:16:50.709966    6755 secure_serving.go:116] Serving securely on 10.11.37.71:10259
Sep 23 20:16:51 k8stian-m1 kube-scheduler: I0923 20:16:51.618441    6755 controller_utils.go:1027] Waiting for caches to sync for scheduler controller
Sep 23 20:16:51 k8stian-m1 kube-scheduler: I0923 20:16:51.718735    6755 controller_utils.go:1034] Caches are synced for scheduler controller
Sep 23 20:16:51 k8stian-m1 kube-scheduler: I0923 20:16:51.718846    6755 leaderelection.go:217] attempting to acquire leader lease  kube-system/kube-scheduler...
Sep 23 20:16:52 k8stian-m1 rsyslogd: imjournal: journal reloaded... [v8.24.0-34.el7 try http://www.rsyslog.com/e/0 ]
Sep 23 20:16:54 k8stian-m1 systemd: kubelet.service holdoff time over, scheduling restart.
Sep 23 20:16:54 k8stian-m1 systemd: Stopped Kubernetes Kubelet.
Sep 23 20:16:54 k8stian-m1 systemd: Started Kubernetes Kubelet.
Sep 23 20:16:55 k8stian-m1 kubelet: Flag --allow-privileged has been deprecated, will be removed in a future version
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.144347    6847 flags.go:33] FLAG: --address="0.0.0.0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150155    6847 flags.go:33] FLAG: --allow-privileged="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150191    6847 flags.go:33] FLAG: --allowed-unsafe-sysctls="[]"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150201    6847 flags.go:33] FLAG: --alsologtostderr="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150208    6847 flags.go:33] FLAG: --anonymous-auth="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150211    6847 flags.go:33] FLAG: --application-metrics-count-limit="100"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150216    6847 flags.go:33] FLAG: --authentication-token-webhook="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150220    6847 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150235    6847 flags.go:33] FLAG: --authorization-mode="AlwaysAllow"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150240    6847 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150244    6847 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150247    6847 flags.go:33] FLAG: --azure-container-registry-config=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150251    6847 flags.go:33] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150255    6847 flags.go:33] FLAG: --bootstrap-checkpoint-path=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150259    6847 flags.go:33] FLAG: --bootstrap-kubeconfig="/etc/kubernetes/kubelet-bootstrap.kubeconfig"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150263    6847 flags.go:33] FLAG: --cert-dir="/etc/kubernetes/cert"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150267    6847 flags.go:33] FLAG: --cgroup-driver="cgroupfs"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150270    6847 flags.go:33] FLAG: --cgroup-root=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150273    6847 flags.go:33] FLAG: --cgroups-per-qos="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150277    6847 flags.go:33] FLAG: --chaos-chance="0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150285    6847 flags.go:33] FLAG: --client-ca-file=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150289    6847 flags.go:33] FLAG: --cloud-config=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150292    6847 flags.go:33] FLAG: --cloud-provider=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150295    6847 flags.go:33] FLAG: --cluster-dns="[]"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150300    6847 flags.go:33] FLAG: --cluster-domain=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150303    6847 flags.go:33] FLAG: --cni-bin-dir="/opt/cni/bin"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150307    6847 flags.go:33] FLAG: --cni-conf-dir="/etc/cni/net.d"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150326    6847 flags.go:33] FLAG: --config="/etc/kubernetes/kubelet-config.yaml"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150329    6847 flags.go:33] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150334    6847 flags.go:33] FLAG: --container-log-max-files="5"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150351    6847 flags.go:33] FLAG: --container-log-max-size="10Mi"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150354    6847 flags.go:33] FLAG: --container-runtime="docker"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150358    6847 flags.go:33] FLAG: --container-runtime-endpoint="unix:///var/run/dockershim.sock"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150404    6847 flags.go:33] FLAG: --containerd="unix:///var/run/containerd.sock"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150414    6847 flags.go:33] FLAG: --containerized="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150417    6847 flags.go:33] FLAG: --contention-profiling="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150421    6847 flags.go:33] FLAG: --cpu-cfs-quota="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150424    6847 flags.go:33] FLAG: --cpu-cfs-quota-period="100ms"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150427    6847 flags.go:33] FLAG: --cpu-manager-policy="none"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150430    6847 flags.go:33] FLAG: --cpu-manager-reconcile-period="10s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150433    6847 flags.go:33] FLAG: --docker="unix:///var/run/docker.sock"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150437    6847 flags.go:33] FLAG: --docker-endpoint="unix:///var/run/docker.sock"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150440    6847 flags.go:33] FLAG: --docker-env-metadata-whitelist=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150444    6847 flags.go:33] FLAG: --docker-only="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150449    6847 flags.go:33] FLAG: --docker-root="/var/lib/docker"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150452    6847 flags.go:33] FLAG: --docker-tls="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150457    6847 flags.go:33] FLAG: --docker-tls-ca="ca.pem"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150462    6847 flags.go:33] FLAG: --docker-tls-cert="cert.pem"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150468    6847 flags.go:33] FLAG: --docker-tls-key="key.pem"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150473    6847 flags.go:33] FLAG: --dynamic-config-dir=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150481    6847 flags.go:33] FLAG: --enable-controller-attach-detach="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150487    6847 flags.go:33] FLAG: --enable-debugging-handlers="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150492    6847 flags.go:33] FLAG: --enable-load-reader="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150497    6847 flags.go:33] FLAG: --enable-server="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150502    6847 flags.go:33] FLAG: --enforce-node-allocatable="[pods]"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150513    6847 flags.go:33] FLAG: --event-burst="10"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150519    6847 flags.go:33] FLAG: --event-qps="5"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150523    6847 flags.go:33] FLAG: --event-storage-age-limit="default=0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150526    6847 flags.go:33] FLAG: --event-storage-event-limit="default=0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150530    6847 flags.go:33] FLAG: --eviction-hard="imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150547    6847 flags.go:33] FLAG: --eviction-max-pod-grace-period="0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150568    6847 flags.go:33] FLAG: --eviction-minimum-reclaim=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150577    6847 flags.go:33] FLAG: --eviction-pressure-transition-period="5m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150583    6847 flags.go:33] FLAG: --eviction-soft=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150589    6847 flags.go:33] FLAG: --eviction-soft-grace-period=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150594    6847 flags.go:33] FLAG: --exit-on-lock-contention="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150600    6847 flags.go:33] FLAG: --experimental-allocatable-ignore-eviction="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150606    6847 flags.go:33] FLAG: --experimental-bootstrap-kubeconfig="/etc/kubernetes/kubelet-bootstrap.kubeconfig"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150613    6847 flags.go:33] FLAG: --experimental-check-node-capabilities-before-mount="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150619    6847 flags.go:33] FLAG: --experimental-dockershim="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150625    6847 flags.go:33] FLAG: --experimental-dockershim-root-directory="/var/lib/dockershim"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150630    6847 flags.go:33] FLAG: --experimental-kernel-memcg-notification="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150635    6847 flags.go:33] FLAG: --experimental-mounter-path=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150640    6847 flags.go:33] FLAG: --fail-swap-on="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150645    6847 flags.go:33] FLAG: --feature-gates=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150652    6847 flags.go:33] FLAG: --file-check-frequency="20s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150655    6847 flags.go:33] FLAG: --global-housekeeping-interval="1m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150659    6847 flags.go:33] FLAG: --hairpin-mode="promiscuous-bridge"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150662    6847 flags.go:33] FLAG: --healthz-bind-address="127.0.0.1"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150665    6847 flags.go:33] FLAG: --healthz-port="10248"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150669    6847 flags.go:33] FLAG: --help="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150673    6847 flags.go:33] FLAG: --host-ipc-sources="[*]"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150681    6847 flags.go:33] FLAG: --host-network-sources="[*]"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150692    6847 flags.go:33] FLAG: --host-pid-sources="[*]"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150698    6847 flags.go:33] FLAG: --hostname-override="k8stian-m1"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150704    6847 flags.go:33] FLAG: --housekeeping-interval="10s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150711    6847 flags.go:33] FLAG: --http-check-frequency="20s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150716    6847 flags.go:33] FLAG: --image-gc-high-threshold="85"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150722    6847 flags.go:33] FLAG: --image-gc-low-threshold="80"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150727    6847 flags.go:33] FLAG: --image-pull-progress-deadline="15m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150734    6847 flags.go:33] FLAG: --image-service-endpoint=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150767    6847 flags.go:33] FLAG: --iptables-drop-bit="15"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150772    6847 flags.go:33] FLAG: --iptables-masquerade-bit="14"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150775    6847 flags.go:33] FLAG: --keep-terminated-pod-volumes="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150778    6847 flags.go:33] FLAG: --kube-api-burst="10"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150781    6847 flags.go:33] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150785    6847 flags.go:33] FLAG: --kube-api-qps="5"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150788    6847 flags.go:33] FLAG: --kube-reserved=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150791    6847 flags.go:33] FLAG: --kube-reserved-cgroup=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150795    6847 flags.go:33] FLAG: --kubeconfig="/etc/kubernetes/kubelet.kubeconfig"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150798    6847 flags.go:33] FLAG: --kubelet-cgroups=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150801    6847 flags.go:33] FLAG: --lock-file=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150803    6847 flags.go:33] FLAG: --log-backtrace-at=":0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150808    6847 flags.go:33] FLAG: --log-cadvisor-usage="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150811    6847 flags.go:33] FLAG: --log-dir=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150813    6847 flags.go:33] FLAG: --log-file=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150816    6847 flags.go:33] FLAG: --log-flush-frequency="5s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150820    6847 flags.go:33] FLAG: --logtostderr="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150823    6847 flags.go:33] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150826    6847 flags.go:33] FLAG: --make-iptables-util-chains="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150829    6847 flags.go:33] FLAG: --manifest-url=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150832    6847 flags.go:33] FLAG: --manifest-url-header=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150839    6847 flags.go:33] FLAG: --master-service-namespace="default"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150843    6847 flags.go:33] FLAG: --max-open-files="1000000"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150848    6847 flags.go:33] FLAG: --max-pods="110"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150851    6847 flags.go:33] FLAG: --maximum-dead-containers="-1"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150868    6847 flags.go:33] FLAG: --maximum-dead-containers-per-container="1"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150872    6847 flags.go:33] FLAG: --minimum-container-ttl-duration="0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150875    6847 flags.go:33] FLAG: --minimum-image-ttl-duration="2m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150878    6847 flags.go:33] FLAG: --network-plugin=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150881    6847 flags.go:33] FLAG: --network-plugin-mtu="0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150883    6847 flags.go:33] FLAG: --node-ip=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150887    6847 flags.go:33] FLAG: --node-labels=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150893    6847 flags.go:33] FLAG: --node-status-max-images="50"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150896    6847 flags.go:33] FLAG: --node-status-update-frequency="10s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150899    6847 flags.go:33] FLAG: --non-masquerade-cidr="10.0.0.0/8"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150902    6847 flags.go:33] FLAG: --oom-score-adj="-999"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150905    6847 flags.go:33] FLAG: --pod-cidr=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150908    6847 flags.go:33] FLAG: --pod-infra-container-image="registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150912    6847 flags.go:33] FLAG: --pod-manifest-path=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150915    6847 flags.go:33] FLAG: --pod-max-pids="-1"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150919    6847 flags.go:33] FLAG: --pods-per-core="0"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150921    6847 flags.go:33] FLAG: --port="10250"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150924    6847 flags.go:33] FLAG: --protect-kernel-defaults="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150927    6847 flags.go:33] FLAG: --provider-id=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150930    6847 flags.go:33] FLAG: --qos-reserved=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150935    6847 flags.go:33] FLAG: --read-only-port="10255"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150938    6847 flags.go:33] FLAG: --really-crash-for-testing="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150941    6847 flags.go:33] FLAG: --redirect-container-streaming="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150944    6847 flags.go:33] FLAG: --register-node="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150947    6847 flags.go:33] FLAG: --register-schedulable="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150950    6847 flags.go:33] FLAG: --register-with-taints=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150954    6847 flags.go:33] FLAG: --registry-burst="10"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150957    6847 flags.go:33] FLAG: --registry-qps="5"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150960    6847 flags.go:33] FLAG: --resolv-conf="/etc/resolv.conf"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150964    6847 flags.go:33] FLAG: --root-dir="/data/k8s/k8s/kubelet"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150967    6847 flags.go:33] FLAG: --rotate-certificates="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150970    6847 flags.go:33] FLAG: --rotate-server-certificates="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150973    6847 flags.go:33] FLAG: --runonce="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150977    6847 flags.go:33] FLAG: --runtime-cgroups=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150979    6847 flags.go:33] FLAG: --runtime-request-timeout="2m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150983    6847 flags.go:33] FLAG: --seccomp-profile-root="/var/lib/kubelet/seccomp"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150986    6847 flags.go:33] FLAG: --serialize-image-pulls="true"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150989    6847 flags.go:33] FLAG: --stderrthreshold="2"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150992    6847 flags.go:33] FLAG: --storage-driver-buffer-duration="1m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150995    6847 flags.go:33] FLAG: --storage-driver-db="cadvisor"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.150998    6847 flags.go:33] FLAG: --storage-driver-host="localhost:8086"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151002    6847 flags.go:33] FLAG: --storage-driver-password="root"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151005    6847 flags.go:33] FLAG: --storage-driver-secure="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151008    6847 flags.go:33] FLAG: --storage-driver-table="stats"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151011    6847 flags.go:33] FLAG: --storage-driver-user="root"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151014    6847 flags.go:33] FLAG: --streaming-connection-idle-timeout="4h0m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151017    6847 flags.go:33] FLAG: --sync-frequency="1m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151020    6847 flags.go:33] FLAG: --system-cgroups=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151023    6847 flags.go:33] FLAG: --system-reserved=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151026    6847 flags.go:33] FLAG: --system-reserved-cgroup=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151029    6847 flags.go:33] FLAG: --tls-cert-file=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151032    6847 flags.go:33] FLAG: --tls-cipher-suites="[]"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151038    6847 flags.go:33] FLAG: --tls-min-version=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151041    6847 flags.go:33] FLAG: --tls-private-key-file=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151043    6847 flags.go:33] FLAG: --v="2"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151046    6847 flags.go:33] FLAG: --version="false"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151052    6847 flags.go:33] FLAG: --vmodule=""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151056    6847 flags.go:33] FLAG: --volume-plugin-dir="/data/k8s/k8s/kubelet/kubelet-plugins/volume/exec/"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151060    6847 flags.go:33] FLAG: --volume-stats-agg-period="1m0s"
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.151094    6847 feature_gate.go:226] feature gates: &{map[]}
Sep 23 20:16:55 k8stian-m1 kubelet: Flag --allow-privileged has been deprecated, will be removed in a future version
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.164513    6847 feature_gate.go:226] feature gates: &{map[]}
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.164559    6847 feature_gate.go:226] feature gates: &{map[]}
Sep 23 20:16:55 k8stian-m1 systemd: kube-controller-manager.service holdoff time over, scheduling restart.
Sep 23 20:16:55 k8stian-m1 systemd: Stopped Kubernetes Controller Manager.
Sep 23 20:16:55 k8stian-m1 systemd: Started Kubernetes Controller Manager.
Sep 23 20:16:55 k8stian-m1 systemd: Started Kubernetes systemd probe.
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.419913    6847 mount_linux.go:171] Detected OS with systemd
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.419999    6847 server.go:418] Version: v1.14.5
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.420096    6847 feature_gate.go:226] feature gates: &{map[]}
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.420198    6847 feature_gate.go:226] feature gates: &{map[]}
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.420342    6847 plugins.go:103] No cloud provider specified.
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.420403    6847 server.go:534] No cloud provider specified: "" from the config file: ""
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.420418    6847 server.go:759] Client rotation is on, will bootstrap in background
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.497311    6847 bootstrap.go:83] Current kubeconfig file contents are still valid, no bootstrap necessary
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.497422    6847 certificate_store.go:130] Loading cert/key pair from "/etc/kubernetes/cert/kubelet-client-current.pem".
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.497896    6847 server.go:782] Starting client certificate rotation.
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.497905    6847 certificate_manager.go:249] Certificate rotation is enabled.
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.498158    6847 certificate_manager.go:489] Certificate expiration is 2119-06-09 02:15:47 +0000 UTC, rotation deadline is 2104-09-07 04:12:07.419290368 +0000 UTC
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.498185    6847 certificate_manager.go:255] Waiting 744711h55m11.921111559s for next certificate rotation
Sep 23 20:16:55 k8stian-m1 kubelet: I0923 20:16:55.498873    6847 manager.go:156] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: Flag --port has been deprecated, see --secure-port instead.
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003491    6868 flags.go:33] FLAG: --address="0.0.0.0"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003537    6868 flags.go:33] FLAG: --allocate-node-cidrs="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003543    6868 flags.go:33] FLAG: --allow-untagged-cloud="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003546    6868 flags.go:33] FLAG: --alsologtostderr="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003551    6868 flags.go:33] FLAG: --attach-detach-reconcile-sync-period="1m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003556    6868 flags.go:33] FLAG: --authentication-kubeconfig="/etc/kubernetes/kube-controller-manager.kubeconfig"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003561    6868 flags.go:33] FLAG: --authentication-skip-lookup="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003565    6868 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl="10s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003568    6868 flags.go:33] FLAG: --authentication-tolerate-lookup-failure="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003571    6868 flags.go:33] FLAG: --authorization-always-allow-paths="[/healthz]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003578    6868 flags.go:33] FLAG: --authorization-kubeconfig="/etc/kubernetes/kube-controller-manager.kubeconfig"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003582    6868 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl="10s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003586    6868 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl="10s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003589    6868 flags.go:33] FLAG: --bind-address="10.11.37.71"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003593    6868 flags.go:33] FLAG: --cert-dir=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003596    6868 flags.go:33] FLAG: --cidr-allocator-type="RangeAllocator"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003600    6868 flags.go:33] FLAG: --client-ca-file="/etc/kubernetes/cert/ca.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003603    6868 flags.go:33] FLAG: --cloud-config=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003606    6868 flags.go:33] FLAG: --cloud-provider=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003609    6868 flags.go:33] FLAG: --cloud-provider-gce-lb-src-cidrs="130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003616    6868 flags.go:33] FLAG: --cluster-cidr=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003619    6868 flags.go:33] FLAG: --cluster-name="kubernetes"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003622    6868 flags.go:33] FLAG: --cluster-signing-cert-file="/etc/kubernetes/cert/ca.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003626    6868 flags.go:33] FLAG: --cluster-signing-key-file="/etc/kubernetes/cert/ca-key.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003630    6868 flags.go:33] FLAG: --concurrent-deployment-syncs="10"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003635    6868 flags.go:33] FLAG: --concurrent-endpoint-syncs="5"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003639    6868 flags.go:33] FLAG: --concurrent-gc-syncs="30"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003642    6868 flags.go:33] FLAG: --concurrent-namespace-syncs="10"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003646    6868 flags.go:33] FLAG: --concurrent-replicaset-syncs="5"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003649    6868 flags.go:33] FLAG: --concurrent-resource-quota-syncs="5"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003652    6868 flags.go:33] FLAG: --concurrent-service-syncs="2"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003655    6868 flags.go:33] FLAG: --concurrent-serviceaccount-token-syncs="5"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003659    6868 flags.go:33] FLAG: --concurrent-ttl-after-finished-syncs="5"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003678    6868 flags.go:33] FLAG: --concurrent_rc_syncs="5"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003681    6868 flags.go:33] FLAG: --configure-cloud-routes="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003685    6868 flags.go:33] FLAG: --contention-profiling="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003688    6868 flags.go:33] FLAG: --controller-start-interval="0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003693    6868 flags.go:33] FLAG: --controllers="[*,bootstrapsigner,tokencleaner]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003700    6868 flags.go:33] FLAG: --deleting-pods-burst="0"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003703    6868 flags.go:33] FLAG: --deleting-pods-qps="0.1"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003709    6868 flags.go:33] FLAG: --deployment-controller-sync-period="30s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003723    6868 flags.go:33] FLAG: --disable-attach-detach-reconcile-sync="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003726    6868 flags.go:33] FLAG: --enable-dynamic-provisioning="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003730    6868 flags.go:33] FLAG: --enable-garbage-collector="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003733    6868 flags.go:33] FLAG: --enable-hostpath-provisioner="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003738    6868 flags.go:33] FLAG: --enable-taint-manager="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003758    6868 flags.go:33] FLAG: --experimental-cluster-signing-duration="876000h0m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003762    6868 flags.go:33] FLAG: --external-cloud-volume-plugin=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003765    6868 flags.go:33] FLAG: --feature-gates=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003771    6868 flags.go:33] FLAG: --flex-volume-plugin-dir="/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003775    6868 flags.go:33] FLAG: --help="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003778    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-cpu-initialization-period="5m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003786    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-downscale-delay="5m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003790    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-downscale-stabilization="5m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003793    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-initial-readiness-delay="30s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003796    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-sync-period="10s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003799    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-tolerance="0.1"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003828    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-upscale-delay="3m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003832    6868 flags.go:33] FLAG: --horizontal-pod-autoscaler-use-rest-clients="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003835    6868 flags.go:33] FLAG: --http2-max-streams-per-connection="0"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003840    6868 flags.go:33] FLAG: --kube-api-burst="2000"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003843    6868 flags.go:33] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003856    6868 flags.go:33] FLAG: --kube-api-qps="1000"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003860    6868 flags.go:33] FLAG: --kubeconfig="/etc/kubernetes/kube-controller-manager.kubeconfig"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003873    6868 flags.go:33] FLAG: --large-cluster-size-threshold="50"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003876    6868 flags.go:33] FLAG: --leader-elect="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003879    6868 flags.go:33] FLAG: --leader-elect-lease-duration="15s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003882    6868 flags.go:33] FLAG: --leader-elect-renew-deadline="10s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003885    6868 flags.go:33] FLAG: --leader-elect-resource-lock="endpoints"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003888    6868 flags.go:33] FLAG: --leader-elect-retry-period="2s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003891    6868 flags.go:33] FLAG: --log-backtrace-at=":0"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003897    6868 flags.go:33] FLAG: --log-dir=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003901    6868 flags.go:33] FLAG: --log-file=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003904    6868 flags.go:33] FLAG: --log-flush-frequency="5s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003907    6868 flags.go:33] FLAG: --logtostderr="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003910    6868 flags.go:33] FLAG: --master=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003913    6868 flags.go:33] FLAG: --min-resync-period="12h0m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003917    6868 flags.go:33] FLAG: --namespace-sync-period="5m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003920    6868 flags.go:33] FLAG: --node-cidr-mask-size="24"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003923    6868 flags.go:33] FLAG: --node-eviction-rate="0.1"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003926    6868 flags.go:33] FLAG: --node-monitor-grace-period="40s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003929    6868 flags.go:33] FLAG: --node-monitor-period="5s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003932    6868 flags.go:33] FLAG: --node-startup-grace-period="1m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003935    6868 flags.go:33] FLAG: --node-sync-period="0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003938    6868 flags.go:33] FLAG: --pod-eviction-timeout="6m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003941    6868 flags.go:33] FLAG: --port="0"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003944    6868 flags.go:33] FLAG: --profiling="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003948    6868 flags.go:33] FLAG: --pv-recycler-increment-timeout-nfs="30"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003951    6868 flags.go:33] FLAG: --pv-recycler-minimum-timeout-hostpath="60"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003955    6868 flags.go:33] FLAG: --pv-recycler-minimum-timeout-nfs="300"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003958    6868 flags.go:33] FLAG: --pv-recycler-pod-template-filepath-hostpath=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003963    6868 flags.go:33] FLAG: --pv-recycler-pod-template-filepath-nfs=""
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.040254    6847 fs.go:144] Filesystem UUIDs: map[4c4bfe58-4b69-40c9-b329-dee0054a27ae:/dev/dm-1 a6132031-acb5-4652-bff5-724b1b358278:/dev/dm-2 cab23e23-ada8-416e-9687-aaf463af83c8:/dev/vda1 ce316201-57b7-4408-a4b1-7bd8b144a8b4:/dev/dm-0]
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.040318    6847 fs.go:145] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/ major:253 minor:0 fsType:xfs blockSize:0} /dev/mapper/vg--data-lv--data:{mountpoint:/data major:253 minor:2 fsType:ext4 blockSize:0} /dev/vda1:{mountpoint:/boot major:252 minor:1 fsType:xfs blockSize:0} shm:{mountpoint:/data/k8s/docker/data/containers/9d6619007c1a8a06f4c9488b9491e5daebae0aef20b2f16a68d75b77edf2f2dc/mounts/shm major:0 minor:48 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:19 fsType:tmpfs blockSize:0}]
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003966    6868 flags.go:33] FLAG: --pv-recycler-timeout-increment-hostpath="30"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003969    6868 flags.go:33] FLAG: --pvclaimbinder-sync-period="15s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003972    6868 flags.go:33] FLAG: --register-retry-count="10"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003975    6868 flags.go:33] FLAG: --requestheader-allowed-names="[]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003979    6868 flags.go:33] FLAG: --requestheader-client-ca-file="/etc/kubernetes/cert/ca.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003983    6868 flags.go:33] FLAG: --requestheader-extra-headers-prefix="[X-Remote-Extra-]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003990    6868 flags.go:33] FLAG: --requestheader-group-headers="[X-Remote-Group]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.003994    6868 flags.go:33] FLAG: --requestheader-username-headers="[X-Remote-User]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004000    6868 flags.go:33] FLAG: --resource-quota-sync-period="5m0s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004003    6868 flags.go:33] FLAG: --root-ca-file="/etc/kubernetes/cert/ca.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004007    6868 flags.go:33] FLAG: --route-reconciliation-period="10s"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004010    6868 flags.go:33] FLAG: --secondary-node-eviction-rate="0.01"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004014    6868 flags.go:33] FLAG: --secure-port="10252"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004017    6868 flags.go:33] FLAG: --service-account-private-key-file="/etc/kubernetes/cert/ca-key.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004021    6868 flags.go:33] FLAG: --service-cluster-ip-range="172.31.0.0/16"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004024    6868 flags.go:33] FLAG: --skip-headers="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004027    6868 flags.go:33] FLAG: --stderrthreshold="2"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004030    6868 flags.go:33] FLAG: --terminated-pod-gc-threshold="10000"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004033    6868 flags.go:33] FLAG: --tls-cert-file="/etc/kubernetes/cert/kube-controller-manager.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004037    6868 flags.go:33] FLAG: --tls-cipher-suites="[]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004041    6868 flags.go:33] FLAG: --tls-min-version=""
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004044    6868 flags.go:33] FLAG: --tls-private-key-file="/etc/kubernetes/cert/kube-controller-manager-key.pem"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004064    6868 flags.go:33] FLAG: --tls-sni-cert-key="[]"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004069    6868 flags.go:33] FLAG: --unhealthy-zone-threshold="0.55"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004072    6868 flags.go:33] FLAG: --use-service-account-credentials="true"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004076    6868 flags.go:33] FLAG: --v="2"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004079    6868 flags.go:33] FLAG: --version="false"
Sep 23 20:16:56 k8stian-m1 kube-controller-manager: I0923 20:16:56.004085    6868 flags.go:33] FLAG: --vmodule=""
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.046461    6847 manager.go:231] Machine: {NumCores:8 CpuFrequency:2399996 MemoryCapacity:8370180096 HugePages:[{PageSize:2048 NumPages:0}] MachineID:293aadfed5d9403ba4536458021a714b SystemUUID:BCB503A5-CEE8-4E30-AB33-897B53CDA75C BootID:cadff69a-f0f0-4878-9fa4-c172211a118a Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:19 Capacity:4185088000 Type:vfs Inodes:1021750 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:18238930944 Type:vfs Inodes:8910848 HasInodes:true} {Device:/dev/vda1 DeviceMajor:252 DeviceMinor:1 Capacity:1063256064 Type:vfs Inodes:524288 HasInodes:true} {Device:/dev/mapper/vg--data-lv--data DeviceMajor:253 DeviceMinor:2 Capacity:105550635008 Type:vfs Inodes:6553600 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:48 Capacity:67108864 Type:vfs Inodes:1021750 HasInodes:true}] DiskMap:map[252:0:{Name:vda Major:252 Minor:0 Size:21474836480 Scheduler:none} 252:16:{Name:vdb Major:252 Minor:16 Size:107374182400 Scheduler:none} 253:0:{Name:dm-0 Major:253 Minor:0 Size:18249416704 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:107369988096 Scheduler:none}] NetworkDevices:[{Name:dummy0 MacAddress:f6:63:26:88:45:3b Speed:0 Mtu:1500} {Name:eth0 MacAddress:52:54:00:4d:af:e5 Speed:0 Mtu:1500} {Name:eth1 MacAddress:52:54:00:fb:9a:86 Speed:0 Mtu:1500} {Name:flannel.1 MacAddress:8e:e5:9e:c7:44:78 Speed:0 Mtu:1450} {Name:kube-ipvs0 MacAddress:9a:02:88:69:1b:4b Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:8370180096 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:3 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:4 Memory:0 Cores:[{Id:0 Threads:[4] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:5 Memory:0 Cores:[{Id:0 Threads:[5] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:6 Memory:0 Cores:[{Id:0 Threads:[6] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:7 Memory:0 Cores:[{Id:0 Threads:[7] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.048171    6847 manager.go:237] Version: {KernelVersion:4.4.184-1.el7.elrepo.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:18.09.7 DockerAPIVersion:1.39 CadvisorVersion: CadvisorRevision:}
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.059190    6847 server.go:629] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.079733    6847 container_manager_linux.go:261] container manager verified user specified cgroup-root exists: []
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.079801    6847 container_manager_linux.go:266] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/data/k8s/k8s/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms}
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.079916    6847 container_manager_linux.go:286] Creating device plugin manager: true
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.079935    6847 manager.go:109] Creating Device Plugin manager at /var/lib/kubelet/device-plugins/kubelet.sock
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.080009    6847 state_mem.go:36] [cpumanager] initializing new in-memory state store
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.097458    6847 state_mem.go:84] [cpumanager] updated default cpuset: ""
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.097486    6847 state_mem.go:92] [cpumanager] updated cpuset assignments: "map[]"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.097505    6847 state_checkpoint.go:100] [cpumanager] state checkpoint: restored state from checkpoint
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.097512    6847 state_checkpoint.go:101] [cpumanager] state checkpoint: defaultCPUSet:
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.097586    6847 server.go:997] Using root directory: /data/k8s/k8s/kubelet
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.097612    6847 kubelet.go:304] Watching apiserver
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.141090    6847 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.141131    6847 client.go:104] Start docker client with request timeout=10m0s
Sep 23 20:16:56 k8stian-m1 kubelet: W0923 20:16:56.151566    6847 docker_service.go:561] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.151603    6847 docker_service.go:238] Hairpin mode set to "hairpin-veth"
Sep 23 20:16:56 k8stian-m1 kubelet: W0923 20:16:56.151740    6847 cni.go:213] Unable to update cni config: No networks found in /etc/cni/net.d
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.461582    6847 docker_service.go:253] Docker cri networking managed by kubernetes.io/no-op
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.521992    6847 docker_service.go:258] Docker Info: &{ID:CVYT:CYF4:NCEX:ASAE:XCTS:DWNC:BC7E:3HF7:FDCV:JNYS:7X3P:BMK6 Containers:21 ContainersRunning:20 ContainersPaused:0 ContainersStopped:1 Images:23 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:true NFd:145 OomKillDisable:true NGoroutines:134 SystemTime:2019-09-23T20:16:56.48326055+08:00 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:4.4.184-1.el7.elrepo.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0003d78f0 NCPU:8 MemTotal:8370180096 GenericResources:[] DockerRootDir:/data/k8s/docker/data HTTPProxy: HTTPSProxy: NoProxy: Name:k8stian-m1 Labels:[] ExperimentalBuild:false ServerVersion:18.09.7 ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:runc Args:[]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil>} LiveRestoreEnabled:true Isolation: InitBinary:docker-init ContainerdCommit:{ID:894b81a4b802e4eb2a91d1ce216b8817763c29fb Expected:894b81a4b802e4eb2a91d1ce216b8817763c29fb} RuncCommit:{ID:425e105d5a03fabd737a126ad93d62a9eeede87f Expected:425e105d5a03fabd737a126ad93d62a9eeede87f} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default]}
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.522130    6847 docker_service.go:271] Setting cgroupDriver to cgroupfs
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.522276    6847 kubelet.go:632] Starting the GRPC server for the docker CRI shim.
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.522335    6847 docker_server.go:59] Start dockershim grpc server
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542006    6847 remote_runtime.go:62] parsed scheme: ""
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542043    6847 remote_runtime.go:62] scheme "" not registered, fallback to default scheme
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542072    6847 remote_image.go:50] parsed scheme: ""
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542077    6847 remote_image.go:50] scheme "" not registered, fallback to default scheme
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542400    6847 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [{/var/run/dockershim.sock 0  <nil>}]
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542415    6847 clientconn.go:796] ClientConn switching balancer to "pick_first"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542487    6847 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc000b54630, CONNECTING
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542504    6847 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [{/var/run/dockershim.sock 0  <nil>}]
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542519    6847 clientconn.go:796] ClientConn switching balancer to "pick_first"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.542570    6847 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc000a5e610, CONNECTING
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.549596    6847 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc000a5e610, READY
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.549601    6847 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc000b54630, READY
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.553156    6847 kuberuntime_manager.go:210] Container runtime docker initialized, version: 18.09.7, apiVersion: 1.39.0
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.553225    6847 kuberuntime_manager.go:950] updating runtime config through cri with podcidr 172.30.0.0/16
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.554014    6847 docker_service.go:353] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:172.30.0.0/16,},}
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.555997    6847 kubelet_network.go:77] Setting Pod CIDR:  -> 172.30.0.0/16
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.556138    6847 certificate_store.go:130] Loading cert/key pair from "/etc/kubernetes/cert/kubelet-server-current.pem".
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568458    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/aws-ebs"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568506    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/empty-dir"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568524    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/gce-pd"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568540    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/git-repo"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568553    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/host-path"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568582    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/nfs"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568592    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/secret"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568600    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/iscsi"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568611    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/glusterfs"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568618    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/rbd"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568626    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/cinder"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568634    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/quobyte"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568669    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/cephfs"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568685    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/downward-api"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568717    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/fc"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568726    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/flocker"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568734    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/azure-file"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568742    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/configmap"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.568750    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/vsphere-volume"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.589396    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/azure-disk"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.589443    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/photon-pd"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.589454    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/projected"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.589487    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/portworx-volume"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.602439    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/scaleio"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.602495    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/local-volume"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.602503    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/storageos"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.602540    6847 plugins.go:617] Loaded volume plugin "kubernetes.io/csi"
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.604753    6847 server.go:1055] Started kubelet
Sep 23 20:16:56 k8stian-m1 kubelet: E0923 20:16:56.604812    6847 kubelet.go:1282] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data in memory cache
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.605350    6847 server.go:141] Starting to listen on 10.11.37.71:10250
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.612615    6847 certificate_manager.go:249] Certificate rotation is enabled.
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.612674    6847 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.612783    6847 server.go:343] Adding debug handlers to kubelet server.
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.614310    6847 certificate_manager.go:489] Certificate expiration is 2119-06-09 02:16:11 +0000 UTC, rotation deadline is 2101-04-11 06:59:12.74504448 +0000 UTC
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.614338    6847 certificate_manager.go:255] Waiting 714834h42m16.13071122s for next certificate rotation
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.627295    6847 status_manager.go:152] Starting to sync pod status with apiserver
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.627686    6847 kubelet.go:1806] Starting kubelet main sync loop.
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.627719    6847 kubelet.go:1823] skipping pod synchronization - [container runtime status check may not have completed yet., PLEG is not healthy: pleg has yet to be successful.]
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.629057    6847 desired_state_of_world_populator.go:130] Desired state populator starts to run
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.629103    6847 volume_manager.go:246] The desired_state_of_world populator starts
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.629117    6847 volume_manager.go:248] Starting Kubelet Volume Manager
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.641234    6847 factory.go:356] Registering Docker factory
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.642482    6847 factory.go:54] Registering systemd factory
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.642720    6847 factory.go:100] Registering Raw factory
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.643460    6847 manager.go:1256] Started watching for new ooms in manager
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.644466    6847 manager.go:369] Starting recovery of all containers
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.714707    6847 kubelet_node_status.go:283] Setting node annotation to enable volume controller attach/detach
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.717543    6847 kubelet_node_status.go:468] Recording NodeHasSufficientMemory event message for node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.717578    6847 kubelet_node_status.go:468] Recording NodeHasNoDiskPressure event message for node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.717592    6847 kubelet_node_status.go:468] Recording NodeHasSufficientPID event message for node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.717625    6847 kubelet_node_status.go:72] Attempting to register node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.730216    6847 kubelet.go:1823] skipping pod synchronization - container runtime status check may not have completed yet.
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.747027    6847 kubelet_node_status.go:114] Node k8stian-m1 was previously registered
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.747053    6847 kubelet_node_status.go:75] Successfully registered node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.767158    6847 kubelet_node_status.go:468] Recording NodeHasSufficientMemory event message for node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.767195    6847 kubelet_node_status.go:468] Recording NodeHasNoDiskPressure event message for node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.767202    6847 kubelet_node_status.go:468] Recording NodeHasSufficientPID event message for node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.767218    6847 kubelet_node_status.go:468] Recording NodeNotReady event message for node k8stian-m1
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.767227    6847 setters.go:521] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2019-09-23 20:16:56.767207427 +0800 CST m=+2.079824716 LastTransitionTime:2019-09-23 20:16:56.767207427 +0800 CST m=+2.079824716 Reason:KubeletNotReady Message:container runtime status check may not have completed yet.}
Sep 23 20:16:56 k8stian-m1 kubelet: I0923 20:16:56.940820    6847 kubelet.go:1823] skipping pod synchronization - container runtime status check may not have completed yet.
Sep 23 20:16:57 k8stian-m1 kube-controller-manager: I0923 20:16:57.212154    6868 controllermanager.go:155] Version: v1.14.5
Sep 23 20:16:57 k8stian-m1 kube-controller-manager: I0923 20:16:57.228949    6868 secure_serving.go:116] Serving securely on 10.11.37.71:10252
Sep 23 20:16:57 k8stian-m1 kube-controller-manager: I0923 20:16:57.229299    6868 leaderelection.go:217] attempting to acquire leader lease  kube-system/kube-controller-manager...
Sep 23 20:16:57 k8stian-m1 kubelet: I0923 20:16:57.351838    6847 kubelet.go:1823] skipping pod synchronization - container runtime status check may not have completed yet.
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.044811    6847 manager.go:374] Recovery completed
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.139377    6847 cpu_manager.go:155] [cpumanager] starting with none policy
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.139399    6847 cpu_manager.go:156] [cpumanager] reconciling every 10s
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.139409    6847 policy_none.go:42] [cpumanager] none policy: Start
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.144294    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.152039    6847 kubelet.go:1823] skipping pod synchronization - container runtime status check may not have completed yet.
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.158202    6847 manager.go:196] Starting Device Plugin manager
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.168382    6847 manager.go:231] Serving device plugin registration server on "/var/lib/kubelet/device-plugins/kubelet.sock"
Sep 23 20:16:58 k8stian-m1 kubelet: I0923 20:16:58.168449    6847 plugin_watcher.go:90] Plugin Watcher Start at /data/k8s/k8s/kubelet/plugins_registry
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.763598    6847 kubelet.go:1885] SyncLoop (ADD, "api"): "node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5), prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5), speaker-r8zq5_metallb-system(6a7e77dd-b46b-11e9-b0a6-5254004dafe5), monitoring-influxdb-7756cddffb-bwhjk_monitoring(38f22cb2-d076-11e9-8012-5254004dafe5), fluentd-es-v2.4.0-j7562_kube-system(6a7d0854-b46b-11e9-b0a6-5254004dafe5), kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5), nfs-client-provisioner-6ff488f674-rwztd_default(859a5584-c7c7-11e9-8012-5254004dafe5)"
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820780    6847 kubelet.go:1930] SyncLoop (PLEG): "nfs-client-provisioner-6ff488f674-rwztd_default(859a5584-c7c7-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"859a5584-c7c7-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"1d6e6dbd6874b041ba405b331fbf91c7e744d43f64c37a0d684c74aa66557d4a"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820860    6847 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"d853767e873ab66aa2db45a3b19e78cbc36b32667e00147298b406a0add3805e"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820880    6847 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"bb0687d84f059cc6b22aec2d57f8a8a004b35ba64e021a6c27f30a2fb299ab12"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820898    6847 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"2c1cea3ecf41edec3d92a52317a2fc12c14cc3a06930be9e4f6d61c4353db2c2"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820915    6847 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"f9c764b9e3241d1b92a54d4e1a71fa2c30b61daa66ce97be14fb04bd0ce37167"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820931    6847 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"87a6fef3870cb6aa9d86b7a838bc8e6b162e0fc2f8e1f11db5944f5752824a6e"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820948    6847 kubelet.go:1930] SyncLoop (PLEG): "fluentd-es-v2.4.0-j7562_kube-system(6a7d0854-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7d0854-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"bd6e49c6ae8df802c23712161ffc103d0d0b4aa1fe08dc39b3c1d323db1a875c"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820967    6847 kubelet.go:1930] SyncLoop (PLEG): "fluentd-es-v2.4.0-j7562_kube-system(6a7d0854-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7d0854-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"e54323e007a26abae1bfe6fbd5274ffb3527863e5cf055210be8832ba4a7e7f3"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.820983    6847 kubelet.go:1930] SyncLoop (PLEG): "node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7820b2-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"8e47e0857f0e9cec5d66cabcdd3c2ebe1207f53fe80bf32460762c2605704fc0"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821000    6847 kubelet.go:1930] SyncLoop (PLEG): "node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7820b2-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"210a9f040bbc8567c8822d49c7b671085e86d540c1e334fa28ede96309704d42"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821016    6847 kubelet.go:1930] SyncLoop (PLEG): "node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7820b2-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"5c401a6bfb07210b5be751a49e3a3041da1600e60bc2c7bf5493744c68fcf29a"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821032    6847 kubelet.go:1930] SyncLoop (PLEG): "speaker-r8zq5_metallb-system(6a7e77dd-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7e77dd-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"b80fe3582bc6351d056f60520c702df1354e7fa8ed15410ba0afac396a1e278e"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821048    6847 kubelet.go:1930] SyncLoop (PLEG): "speaker-r8zq5_metallb-system(6a7e77dd-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7e77dd-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"9d6619007c1a8a06f4c9488b9491e5daebae0aef20b2f16a68d75b77edf2f2dc"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821063    6847 kubelet.go:1930] SyncLoop (PLEG): "monitoring-influxdb-7756cddffb-bwhjk_monitoring(38f22cb2-d076-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"38f22cb2-d076-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"ed594b69a5db40142ffd77cc6290eaab557544df08d9956b8bb0d44260e839e1"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821081    6847 kubelet.go:1930] SyncLoop (PLEG): "monitoring-influxdb-7756cddffb-bwhjk_monitoring(38f22cb2-d076-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"38f22cb2-d076-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"38a3a110ec9e7bee26ca708b041680495c3fabb87a80c96157557eec768d6b4a"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821097    6847 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"3e1772ea8c8938b51d8b6568bfca34b4b1253a0f438aa6a28d71be4122cf9f56"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821114    6847 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"825171dca3ada43b511089d384e7f2c8d3eb33fe2db84714ae064109fdad94d4"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821129    6847 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"4821e138ebc9cf4cdf596688074a9dcacb625dce8138a61c194c6fbae873c62d"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821148    6847 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerDied", Data:"e8fb76e27c61cd83762f07b60e9fb928a085ba15a3d5f0e8cdbd8f3a1885ba86"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.821222    6847 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"e4654eda586be529843dfbd5722e116097f4fd857700312f6ce5b5d86220b667"}
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.887553    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-0a3b5547-cff5-11e9-931b-52540084153b" (UniqueName: "kubernetes.io/nfs/38f22cb2-d076-11e9-8012-5254004dafe5-pvc-0a3b5547-cff5-11e9-931b-52540084153b") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.987821    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-526xt" (UniqueName: "kubernetes.io/secret/38f22cb2-d076-11e9-8012-5254004dafe5-default-token-526xt") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.987891    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "proc" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-proc") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.987938    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "speaker-token-8k6ct" (UniqueName: "kubernetes.io/secret/6a7e77dd-b46b-11e9-b0a6-5254004dafe5-speaker-token-8k6ct") pod "speaker-r8zq5" (UID: "6a7e77dd-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.988883    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "nfs-client-provisioner-token-6kd7d" (UniqueName: "kubernetes.io/secret/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-provisioner-token-6kd7d") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.989070    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "secret-controller-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-controller-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.989154    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-token-ncfk5" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-token-ncfk5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.989224    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "pvc-0a3b5547-cff5-11e9-931b-52540084153b" (UniqueName: "kubernetes.io/nfs/38f22cb2-d076-11e9-8012-5254004dafe5-pvc-0a3b5547-cff5-11e9-931b-52540084153b") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990112    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "node-exporter-token-cn8fv" (UniqueName: "kubernetes.io/secret/6a7820b2-b46b-11e9-b0a6-5254004dafe5-node-exporter-token-cn8fv") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990146    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-config") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990170    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/98f00373-cf8d-11e9-8012-5254004dafe5-config-out") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990193    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990215    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config-volume" (UniqueName: "kubernetes.io/configmap/6a7d0854-b46b-11e9-b0a6-5254004dafe5-config-volume") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990261    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-state-metrics-token-hd744" (UniqueName: "kubernetes.io/secret/23f67d94-b750-11e9-b0a6-5254004dafe5-kube-state-metrics-token-hd744") pod "kube-state-metrics-7744645f56-brxz4" (UID: "23f67d94-b750-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990312    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-98ef3446-cf8d-11e9-8012-5254004dafe5" (UniqueName: "kubernetes.io/nfs/98f00373-cf8d-11e9-8012-5254004dafe5-pvc-98ef3446-cf8d-11e9-8012-5254004dafe5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990336    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "sys" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-sys") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990354    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "root" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-root") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990374    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "varlog" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlog") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990394    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "varlibdockercontainers" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlibdockercontainers") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990414    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "fluentd-es-token-f982w" (UniqueName: "kubernetes.io/secret/6a7d0854-b46b-11e9-b0a6-5254004dafe5-fluentd-es-token-f982w") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:16:59 k8stian-m1 kubelet: I0923 20:16:59.990505    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "pvc-0a3b5547-cff5-11e9-931b-52540084153b" (UniqueName: "kubernetes.io/nfs/38f22cb2-d076-11e9-8012-5254004dafe5-pvc-0a3b5547-cff5-11e9-931b-52540084153b") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.015194    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "nfs-client-root" (UniqueName: "kubernetes.io/nfs/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-root") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.015249    6847 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "secret-etcd-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-etcd-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.015271    6847 reconciler.go:154] Reconciler: start to sync state
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.242956    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "sys" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-sys") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243016    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "root" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-root") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243041    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "varlog" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlog") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243063    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "varlibdockercontainers" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlibdockercontainers") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243095    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "fluentd-es-token-f982w" (UniqueName: "kubernetes.io/secret/6a7d0854-b46b-11e9-b0a6-5254004dafe5-fluentd-es-token-f982w") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243117    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "nfs-client-root" (UniqueName: "kubernetes.io/nfs/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-root") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243139    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "secret-etcd-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-etcd-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243163    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "default-token-526xt" (UniqueName: "kubernetes.io/secret/38f22cb2-d076-11e9-8012-5254004dafe5-default-token-526xt") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243184    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "proc" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-proc") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243204    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "speaker-token-8k6ct" (UniqueName: "kubernetes.io/secret/6a7e77dd-b46b-11e9-b0a6-5254004dafe5-speaker-token-8k6ct") pod "speaker-r8zq5" (UID: "6a7e77dd-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243227    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "nfs-client-provisioner-token-6kd7d" (UniqueName: "kubernetes.io/secret/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-provisioner-token-6kd7d") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243248    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "secret-controller-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-controller-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243268    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "prometheus-k8s-token-ncfk5" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-token-ncfk5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243297    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "node-exporter-token-cn8fv" (UniqueName: "kubernetes.io/secret/6a7820b2-b46b-11e9-b0a6-5254004dafe5-node-exporter-token-cn8fv") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243315    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "config" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-config") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243338    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/98f00373-cf8d-11e9-8012-5254004dafe5-config-out") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243360    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243380    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "config-volume" (UniqueName: "kubernetes.io/configmap/6a7d0854-b46b-11e9-b0a6-5254004dafe5-config-volume") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243400    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "kube-state-metrics-token-hd744" (UniqueName: "kubernetes.io/secret/23f67d94-b750-11e9-b0a6-5254004dafe5-kube-state-metrics-token-hd744") pod "kube-state-metrics-7744645f56-brxz4" (UID: "23f67d94-b750-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.243424    6847 reconciler.go:252] operationExecutor.MountVolume started for volume "pvc-98ef3446-cf8d-11e9-8012-5254004dafe5" (UniqueName: "kubernetes.io/nfs/98f00373-cf8d-11e9-8012-5254004dafe5-pvc-98ef3446-cf8d-11e9-8012-5254004dafe5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.244233    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "pvc-98ef3446-cf8d-11e9-8012-5254004dafe5" (UniqueName: "kubernetes.io/nfs/98f00373-cf8d-11e9-8012-5254004dafe5-pvc-98ef3446-cf8d-11e9-8012-5254004dafe5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.245119    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "root" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-root") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.245188    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "sys" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-sys") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.245938    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "varlog" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlog") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.247873    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "nfs-client-root" (UniqueName: "kubernetes.io/nfs/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-root") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.259706    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/98f00373-cf8d-11e9-8012-5254004dafe5-config-out") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.265733    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "varlibdockercontainers" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlibdockercontainers") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.265954    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "proc" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-proc") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.270609    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "kube-state-metrics-token-hd744" (UniqueName: "kubernetes.io/secret/23f67d94-b750-11e9-b0a6-5254004dafe5-kube-state-metrics-token-hd744") pod "kube-state-metrics-7744645f56-brxz4" (UID: "23f67d94-b750-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.272932    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.274347    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "nfs-client-provisioner-token-6kd7d" (UniqueName: "kubernetes.io/secret/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-provisioner-token-6kd7d") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.275175    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "secret-controller-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-controller-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.275579    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "prometheus-k8s-token-ncfk5" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-token-ncfk5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.276321    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "config" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-config") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.276678    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "secret-etcd-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-etcd-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.292901    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "default-token-526xt" (UniqueName: "kubernetes.io/secret/38f22cb2-d076-11e9-8012-5254004dafe5-default-token-526xt") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.299281    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "fluentd-es-token-f982w" (UniqueName: "kubernetes.io/secret/6a7d0854-b46b-11e9-b0a6-5254004dafe5-fluentd-es-token-f982w") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.300417    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "node-exporter-token-cn8fv" (UniqueName: "kubernetes.io/secret/6a7820b2-b46b-11e9-b0a6-5254004dafe5-node-exporter-token-cn8fv") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.305064    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "speaker-token-8k6ct" (UniqueName: "kubernetes.io/secret/6a7e77dd-b46b-11e9-b0a6-5254004dafe5-speaker-token-8k6ct") pod "speaker-r8zq5" (UID: "6a7e77dd-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:00 k8stian-m1 kubelet: I0923 20:17:00.321381    6847 operation_generator.go:669] MountVolume.SetUp succeeded for volume "config-volume" (UniqueName: "kubernetes.io/configmap/6a7d0854-b46b-11e9-b0a6-5254004dafe5-config-volume") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 20:17:07 k8stian-m1 kubelet: I0923 20:17:07.012168    6847 kubelet_node_status.go:468] Recording NodeReady event message for node k8stian-m1
Sep 23 20:20:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 20:20:01 k8stian-m1 systemd: Started Session 14174 of user root.
Sep 23 20:20:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 20:21:56 k8stian-m1 kubelet: I0923 20:21:56.627840    6847 kubelet.go:1292] Image garbage collection succeeded
Sep 23 20:21:58 k8stian-m1 kubelet: I0923 20:21:58.148377    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:26:58 k8stian-m1 kubelet: I0923 20:26:58.148785    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:30:02 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 20:30:02 k8stian-m1 systemd: Started Session 14175 of user root.
Sep 23 20:30:02 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 20:31:58 k8stian-m1 kubelet: I0923 20:31:58.149137    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:36:58 k8stian-m1 kubelet: I0923 20:36:58.149496    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:40:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 20:40:01 k8stian-m1 systemd: Started Session 14176 of user root.
Sep 23 20:40:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 20:41:58 k8stian-m1 kubelet: I0923 20:41:58.149801    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:46:58 k8stian-m1 kubelet: I0923 20:46:58.150165    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:47:07 k8stian-m1 kubelet: W0923 20:47:07.857543    6847 reflector.go:289] object-"monitoring"/"prometheus-k8s-rulefiles-0": watch of *v1.ConfigMap ended with: too old resource version: 15608751 (15609058)
Sep 23 20:48:52 k8stian-m1 kubelet: W0923 20:48:52.853943    6847 reflector.go:289] object-"kube-system"/"fluentd-es-config-v0.2.0": watch of *v1.ConfigMap ended with: too old resource version: 15608751 (15609331)
Sep 23 20:50:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 20:50:01 k8stian-m1 systemd: Started Session 14177 of user root.
Sep 23 20:50:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 20:51:58 k8stian-m1 kubelet: I0923 20:51:58.150571    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 20:56:58 k8stian-m1 kubelet: I0923 20:56:58.151091    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:00:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 21:00:01 k8stian-m1 systemd: Started Session 14178 of user root.
Sep 23 21:00:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 21:01:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 21:01:01 k8stian-m1 systemd: Started Session 14179 of user root.
Sep 23 21:01:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 21:01:58 k8stian-m1 kubelet: I0923 21:01:58.151462    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:06:58 k8stian-m1 kubelet: I0923 21:06:58.153971    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:10:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 21:10:01 k8stian-m1 systemd: Started Session 14180 of user root.
Sep 23 21:10:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 21:11:58 k8stian-m1 kubelet: I0923 21:11:58.154395    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:16:58 k8stian-m1 kubelet: I0923 21:16:58.154801    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:17:00 k8stian-m1 kubelet: W0923 21:17:00.896840    6847 reflector.go:289] object-"monitoring"/"prometheus-k8s-rulefiles-0": watch of *v1.ConfigMap ended with: too old resource version: 15613466 (15613664)
Sep 23 21:17:44 k8stian-m1 kubelet: W0923 21:17:44.868215    6847 reflector.go:289] object-"kube-system"/"fluentd-es-config-v0.2.0": watch of *v1.ConfigMap ended with: too old resource version: 15613733 (15613777)
Sep 23 21:20:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 21:20:01 k8stian-m1 systemd: Started Session 14181 of user root.
Sep 23 21:20:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 21:21:58 k8stian-m1 kubelet: I0923 21:21:58.155232    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:26:58 k8stian-m1 kubelet: I0923 21:26:58.155694    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:30:01 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 21:30:01 k8stian-m1 systemd: Started Session 14182 of user root.
Sep 23 21:30:01 k8stian-m1 systemd: Removed slice User Slice of root.
Sep 23 21:31:58 k8stian-m1 kubelet: I0923 21:31:58.156114    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:36:58 k8stian-m1 kubelet: I0923 21:36:58.156516    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:37:09 k8stian-m1 systemd: Created slice User Slice of root.
Sep 23 21:37:09 k8stian-m1 systemd-logind: New session 14183 of user root.
Sep 23 21:37:09 k8stian-m1 systemd: Started Session 14183 of user root.
Sep 23 21:40:01 k8stian-m1 systemd: Started Session 14184 of user root.
Sep 23 21:41:58 k8stian-m1 kubelet: I0923 21:41:58.156969    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:46:58 k8stian-m1 kubelet: I0923 21:46:58.157296    6847 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:49:05 k8stian-m1 kubelet: W0923 21:49:05.933552    6847 reflector.go:289] object-"kube-system"/"fluentd-es-config-v0.2.0": watch of *v1.ConfigMap ended with: too old resource version: 15618249 (15618660)
Sep 23 21:49:53 k8stian-m1 systemd: flanneld.service holdoff time over, scheduling restart.
Sep 23 21:49:53 k8stian-m1 systemd: Stopping Kubernetes Kubelet...
Sep 23 21:49:53 k8stian-m1 systemd: Stopped Kubernetes Kubelet.
Sep 23 21:49:53 k8stian-m1 systemd: Stopping Docker Application Container Engine...
Sep 23 21:49:55 k8stian-m1 systemd: Stopped Docker Application Container Engine.
Sep 23 21:49:55 k8stian-m1 systemd: Stopped Flanneld overlay address etcd agent.
Sep 23 21:49:55 k8stian-m1 systemd: Starting Flanneld overlay address etcd agent...
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.204034   15394 main.go:527] Using interface with name eth0 and address 10.11.37.71
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.204082   15394 main.go:544] Defaulting external address to interface address (10.11.37.71)
Sep 23 21:49:55 k8stian-m1 flanneld: warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.307879   15394 main.go:244] Created subnet manager: Etcd Local Manager with Previous Subnet: 172.30.48.0/21
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.307890   15394 main.go:247] Installing signal handlers
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.321038   15394 main.go:386] Found network config - Backend type: vxlan
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.321131   15394 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.326625   15394 local_manager.go:147] Found lease (172.30.48.0/21) for current IP (10.11.37.71), reusing
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.330991   15394 main.go:317] Wrote subnet file to /run/flannel/subnet.env
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.331006   15394 main.go:321] Running backend.
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.331175   15394 vxlan_network.go:60] watching for new subnet leases
Sep 23 21:49:55 k8stian-m1 flanneld: I0923 21:49:55.337384   15394 main.go:429] Waiting for 22h59m59.989667455s to renew lease
Sep 23 21:49:55 k8stian-m1 systemd: Started Flanneld overlay address etcd agent.
Sep 23 21:49:55 k8stian-m1 systemd: Started Docker Application Container Engine.
Sep 23 21:49:55 k8stian-m1 systemd: Started Kubernetes Kubelet.
Sep 23 21:49:55 k8stian-m1 kubelet: Flag --allow-privileged has been deprecated, will be removed in a future version
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679080   15437 flags.go:33] FLAG: --address="0.0.0.0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679139   15437 flags.go:33] FLAG: --allow-privileged="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679145   15437 flags.go:33] FLAG: --allowed-unsafe-sysctls="[]"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679153   15437 flags.go:33] FLAG: --alsologtostderr="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679158   15437 flags.go:33] FLAG: --anonymous-auth="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679162   15437 flags.go:33] FLAG: --application-metrics-count-limit="100"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679165   15437 flags.go:33] FLAG: --authentication-token-webhook="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679169   15437 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679173   15437 flags.go:33] FLAG: --authorization-mode="AlwaysAllow"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679178   15437 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679182   15437 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679185   15437 flags.go:33] FLAG: --azure-container-registry-config=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679188   15437 flags.go:33] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679192   15437 flags.go:33] FLAG: --bootstrap-checkpoint-path=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679195   15437 flags.go:33] FLAG: --bootstrap-kubeconfig="/etc/kubernetes/kubelet-bootstrap.kubeconfig"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679199   15437 flags.go:33] FLAG: --cert-dir="/etc/kubernetes/cert"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679202   15437 flags.go:33] FLAG: --cgroup-driver="cgroupfs"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679220   15437 flags.go:33] FLAG: --cgroup-root=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679223   15437 flags.go:33] FLAG: --cgroups-per-qos="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679227   15437 flags.go:33] FLAG: --chaos-chance="0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679233   15437 flags.go:33] FLAG: --client-ca-file=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679236   15437 flags.go:33] FLAG: --cloud-config=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679239   15437 flags.go:33] FLAG: --cloud-provider=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679242   15437 flags.go:33] FLAG: --cluster-dns="[]"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679246   15437 flags.go:33] FLAG: --cluster-domain=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679249   15437 flags.go:33] FLAG: --cni-bin-dir="/opt/cni/bin"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679255   15437 flags.go:33] FLAG: --cni-conf-dir="/etc/cni/net.d"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679259   15437 flags.go:33] FLAG: --config="/etc/kubernetes/kubelet-config.yaml"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679263   15437 flags.go:33] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679267   15437 flags.go:33] FLAG: --container-log-max-files="5"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679271   15437 flags.go:33] FLAG: --container-log-max-size="10Mi"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679275   15437 flags.go:33] FLAG: --container-runtime="docker"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679279   15437 flags.go:33] FLAG: --container-runtime-endpoint="unix:///var/run/dockershim.sock"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679283   15437 flags.go:33] FLAG: --containerd="unix:///var/run/containerd.sock"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679287   15437 flags.go:33] FLAG: --containerized="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679290   15437 flags.go:33] FLAG: --contention-profiling="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679293   15437 flags.go:33] FLAG: --cpu-cfs-quota="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679296   15437 flags.go:33] FLAG: --cpu-cfs-quota-period="100ms"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679300   15437 flags.go:33] FLAG: --cpu-manager-policy="none"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679303   15437 flags.go:33] FLAG: --cpu-manager-reconcile-period="10s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679308   15437 flags.go:33] FLAG: --docker="unix:///var/run/docker.sock"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679312   15437 flags.go:33] FLAG: --docker-endpoint="unix:///var/run/docker.sock"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679315   15437 flags.go:33] FLAG: --docker-env-metadata-whitelist=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679318   15437 flags.go:33] FLAG: --docker-only="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679321   15437 flags.go:33] FLAG: --docker-root="/var/lib/docker"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679325   15437 flags.go:33] FLAG: --docker-tls="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679329   15437 flags.go:33] FLAG: --docker-tls-ca="ca.pem"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679349   15437 flags.go:33] FLAG: --docker-tls-cert="cert.pem"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679352   15437 flags.go:33] FLAG: --docker-tls-key="key.pem"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679356   15437 flags.go:33] FLAG: --dynamic-config-dir=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679361   15437 flags.go:33] FLAG: --enable-controller-attach-detach="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679364   15437 flags.go:33] FLAG: --enable-debugging-handlers="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679367   15437 flags.go:33] FLAG: --enable-load-reader="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679370   15437 flags.go:33] FLAG: --enable-server="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679374   15437 flags.go:33] FLAG: --enforce-node-allocatable="[pods]"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679380   15437 flags.go:33] FLAG: --event-burst="10"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679383   15437 flags.go:33] FLAG: --event-qps="5"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679387   15437 flags.go:33] FLAG: --event-storage-age-limit="default=0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679390   15437 flags.go:33] FLAG: --event-storage-event-limit="default=0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679394   15437 flags.go:33] FLAG: --eviction-hard="imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679406   15437 flags.go:33] FLAG: --eviction-max-pod-grace-period="0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679410   15437 flags.go:33] FLAG: --eviction-minimum-reclaim=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679415   15437 flags.go:33] FLAG: --eviction-pressure-transition-period="5m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679419   15437 flags.go:33] FLAG: --eviction-soft=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679422   15437 flags.go:33] FLAG: --eviction-soft-grace-period=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679425   15437 flags.go:33] FLAG: --exit-on-lock-contention="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679447   15437 flags.go:33] FLAG: --experimental-allocatable-ignore-eviction="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679451   15437 flags.go:33] FLAG: --experimental-bootstrap-kubeconfig="/etc/kubernetes/kubelet-bootstrap.kubeconfig"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679498   15437 flags.go:33] FLAG: --experimental-check-node-capabilities-before-mount="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679502   15437 flags.go:33] FLAG: --experimental-dockershim="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679505   15437 flags.go:33] FLAG: --experimental-dockershim-root-directory="/var/lib/dockershim"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679508   15437 flags.go:33] FLAG: --experimental-kernel-memcg-notification="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679511   15437 flags.go:33] FLAG: --experimental-mounter-path=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679514   15437 flags.go:33] FLAG: --fail-swap-on="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679517   15437 flags.go:33] FLAG: --feature-gates=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679522   15437 flags.go:33] FLAG: --file-check-frequency="20s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679525   15437 flags.go:33] FLAG: --global-housekeeping-interval="1m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679528   15437 flags.go:33] FLAG: --hairpin-mode="promiscuous-bridge"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679531   15437 flags.go:33] FLAG: --healthz-bind-address="127.0.0.1"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679534   15437 flags.go:33] FLAG: --healthz-port="10248"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679537   15437 flags.go:33] FLAG: --help="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679540   15437 flags.go:33] FLAG: --host-ipc-sources="[*]"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679545   15437 flags.go:33] FLAG: --host-network-sources="[*]"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679551   15437 flags.go:33] FLAG: --host-pid-sources="[*]"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679554   15437 flags.go:33] FLAG: --hostname-override="k8stian-m1"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679557   15437 flags.go:33] FLAG: --housekeeping-interval="10s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679562   15437 flags.go:33] FLAG: --http-check-frequency="20s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679565   15437 flags.go:33] FLAG: --image-gc-high-threshold="85"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679568   15437 flags.go:33] FLAG: --image-gc-low-threshold="80"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679570   15437 flags.go:33] FLAG: --image-pull-progress-deadline="15m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679573   15437 flags.go:33] FLAG: --image-service-endpoint=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679577   15437 flags.go:33] FLAG: --iptables-drop-bit="15"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679579   15437 flags.go:33] FLAG: --iptables-masquerade-bit="14"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679582   15437 flags.go:33] FLAG: --keep-terminated-pod-volumes="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679585   15437 flags.go:33] FLAG: --kube-api-burst="10"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679588   15437 flags.go:33] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Sep 23 21:49:55 k8stian-m1 systemd: Started Kubernetes systemd probe.
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679591   15437 flags.go:33] FLAG: --kube-api-qps="5"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679594   15437 flags.go:33] FLAG: --kube-reserved=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679597   15437 flags.go:33] FLAG: --kube-reserved-cgroup=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679600   15437 flags.go:33] FLAG: --kubeconfig="/etc/kubernetes/kubelet.kubeconfig"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679603   15437 flags.go:33] FLAG: --kubelet-cgroups=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679606   15437 flags.go:33] FLAG: --lock-file=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679609   15437 flags.go:33] FLAG: --log-backtrace-at=":0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679612   15437 flags.go:33] FLAG: --log-cadvisor-usage="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679615   15437 flags.go:33] FLAG: --log-dir=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679618   15437 flags.go:33] FLAG: --log-file=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679621   15437 flags.go:33] FLAG: --log-flush-frequency="5s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679624   15437 flags.go:33] FLAG: --logtostderr="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679627   15437 flags.go:33] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679630   15437 flags.go:33] FLAG: --make-iptables-util-chains="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679633   15437 flags.go:33] FLAG: --manifest-url=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679636   15437 flags.go:33] FLAG: --manifest-url-header=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679641   15437 flags.go:33] FLAG: --master-service-namespace="default"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679644   15437 flags.go:33] FLAG: --max-open-files="1000000"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679648   15437 flags.go:33] FLAG: --max-pods="110"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679651   15437 flags.go:33] FLAG: --maximum-dead-containers="-1"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679654   15437 flags.go:33] FLAG: --maximum-dead-containers-per-container="1"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679657   15437 flags.go:33] FLAG: --minimum-container-ttl-duration="0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679678   15437 flags.go:33] FLAG: --minimum-image-ttl-duration="2m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679681   15437 flags.go:33] FLAG: --network-plugin=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679684   15437 flags.go:33] FLAG: --network-plugin-mtu="0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679702   15437 flags.go:33] FLAG: --node-ip=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679705   15437 flags.go:33] FLAG: --node-labels=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679710   15437 flags.go:33] FLAG: --node-status-max-images="50"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679722   15437 flags.go:33] FLAG: --node-status-update-frequency="10s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679725   15437 flags.go:33] FLAG: --non-masquerade-cidr="10.0.0.0/8"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679727   15437 flags.go:33] FLAG: --oom-score-adj="-999"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679730   15437 flags.go:33] FLAG: --pod-cidr=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679733   15437 flags.go:33] FLAG: --pod-infra-container-image="registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679737   15437 flags.go:33] FLAG: --pod-manifest-path=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679739   15437 flags.go:33] FLAG: --pod-max-pids="-1"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679742   15437 flags.go:33] FLAG: --pods-per-core="0"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679747   15437 flags.go:33] FLAG: --port="10250"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679750   15437 flags.go:33] FLAG: --protect-kernel-defaults="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679753   15437 flags.go:33] FLAG: --provider-id=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679755   15437 flags.go:33] FLAG: --qos-reserved=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679775   15437 flags.go:33] FLAG: --read-only-port="10255"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679778   15437 flags.go:33] FLAG: --really-crash-for-testing="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679782   15437 flags.go:33] FLAG: --redirect-container-streaming="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679785   15437 flags.go:33] FLAG: --register-node="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679788   15437 flags.go:33] FLAG: --register-schedulable="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679791   15437 flags.go:33] FLAG: --register-with-taints=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679796   15437 flags.go:33] FLAG: --registry-burst="10"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679799   15437 flags.go:33] FLAG: --registry-qps="5"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679802   15437 flags.go:33] FLAG: --resolv-conf="/etc/resolv.conf"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.679821   15437 flags.go:33] FLAG: --root-dir="/data/k8s/k8s/kubelet"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680376   15437 flags.go:33] FLAG: --rotate-certificates="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680402   15437 flags.go:33] FLAG: --rotate-server-certificates="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680406   15437 flags.go:33] FLAG: --runonce="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680412   15437 flags.go:33] FLAG: --runtime-cgroups=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680416   15437 flags.go:33] FLAG: --runtime-request-timeout="2m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680420   15437 flags.go:33] FLAG: --seccomp-profile-root="/var/lib/kubelet/seccomp"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680439   15437 flags.go:33] FLAG: --serialize-image-pulls="true"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680443   15437 flags.go:33] FLAG: --stderrthreshold="2"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680446   15437 flags.go:33] FLAG: --storage-driver-buffer-duration="1m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680450   15437 flags.go:33] FLAG: --storage-driver-db="cadvisor"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680463   15437 flags.go:33] FLAG: --storage-driver-host="localhost:8086"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680467   15437 flags.go:33] FLAG: --storage-driver-password="root"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680471   15437 flags.go:33] FLAG: --storage-driver-secure="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680474   15437 flags.go:33] FLAG: --storage-driver-table="stats"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680505   15437 flags.go:33] FLAG: --storage-driver-user="root"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680513   15437 flags.go:33] FLAG: --streaming-connection-idle-timeout="4h0m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680676   15437 flags.go:33] FLAG: --sync-frequency="1m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680684   15437 flags.go:33] FLAG: --system-cgroups=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680687   15437 flags.go:33] FLAG: --system-reserved=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680691   15437 flags.go:33] FLAG: --system-reserved-cgroup=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680694   15437 flags.go:33] FLAG: --tls-cert-file=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680697   15437 flags.go:33] FLAG: --tls-cipher-suites="[]"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680705   15437 flags.go:33] FLAG: --tls-min-version=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680709   15437 flags.go:33] FLAG: --tls-private-key-file=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680712   15437 flags.go:33] FLAG: --v="2"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680715   15437 flags.go:33] FLAG: --version="false"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680722   15437 flags.go:33] FLAG: --vmodule=""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680726   15437 flags.go:33] FLAG: --volume-plugin-dir="/data/k8s/k8s/kubelet/kubelet-plugins/volume/exec/"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680730   15437 flags.go:33] FLAG: --volume-stats-agg-period="1m0s"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.680760   15437 feature_gate.go:226] feature gates: &{map[]}
Sep 23 21:49:55 k8stian-m1 kubelet: Flag --allow-privileged has been deprecated, will be removed in a future version
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.691594   15437 feature_gate.go:226] feature gates: &{map[]}
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.691645   15437 feature_gate.go:226] feature gates: &{map[]}
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.723586   15437 mount_linux.go:171] Detected OS with systemd
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.723632   15437 server.go:418] Version: v1.14.5
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.723707   15437 feature_gate.go:226] feature gates: &{map[]}
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.723751   15437 feature_gate.go:226] feature gates: &{map[]}
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.723857   15437 plugins.go:103] No cloud provider specified.
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.723875   15437 server.go:534] No cloud provider specified: "" from the config file: ""
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.723884   15437 server.go:759] Client rotation is on, will bootstrap in background
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.741100   15437 bootstrap.go:83] Current kubeconfig file contents are still valid, no bootstrap necessary
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.741255   15437 certificate_store.go:130] Loading cert/key pair from "/etc/kubernetes/cert/kubelet-client-current.pem".
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.741686   15437 server.go:782] Starting client certificate rotation.
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.741760   15437 certificate_manager.go:249] Certificate rotation is enabled.
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.742557   15437 manager.go:156] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.743748   15437 certificate_manager.go:489] Certificate expiration is 2119-06-09 02:15:47 +0000 UTC, rotation deadline is 2092-08-26 01:39:08.27676248 +0000 UTC
Sep 23 21:49:55 k8stian-m1 kubelet: I0923 21:49:55.743803   15437 certificate_manager.go:255] Waiting 639251h49m12.532983593s for next certificate rotation
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.505797   15437 fs.go:144] Filesystem UUIDs: map[4c4bfe58-4b69-40c9-b329-dee0054a27ae:/dev/dm-1 a6132031-acb5-4652-bff5-724b1b358278:/dev/dm-2 cab23e23-ada8-416e-9687-aaf463af83c8:/dev/vda1 ce316201-57b7-4408-a4b1-7bd8b144a8b4:/dev/dm-0]
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.505825   15437 fs.go:145] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/ major:253 minor:0 fsType:xfs blockSize:0} /dev/mapper/vg--data-lv--data:{mountpoint:/data major:253 minor:2 fsType:ext4 blockSize:0} /dev/vda1:{mountpoint:/boot major:252 minor:1 fsType:xfs blockSize:0} shm:{mountpoint:/data/k8s/docker/data/containers/9d6619007c1a8a06f4c9488b9491e5daebae0aef20b2f16a68d75b77edf2f2dc/mounts/shm major:0 minor:48 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:19 fsType:tmpfs blockSize:0}]
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.511037   15437 manager.go:231] Machine: {NumCores:8 CpuFrequency:2399996 MemoryCapacity:8370180096 HugePages:[{PageSize:2048 NumPages:0}] MachineID:293aadfed5d9403ba4536458021a714b SystemUUID:BCB503A5-CEE8-4E30-AB33-897B53CDA75C BootID:cadff69a-f0f0-4878-9fa4-c172211a118a Filesystems:[{Device:/dev/mapper/vg--data-lv--data DeviceMajor:253 DeviceMinor:2 Capacity:105550635008 Type:vfs Inodes:6553600 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:48 Capacity:67108864 Type:vfs Inodes:1021750 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:19 Capacity:4185088000 Type:vfs Inodes:1021750 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:18238930944 Type:vfs Inodes:8910848 HasInodes:true} {Device:/dev/vda1 DeviceMajor:252 DeviceMinor:1 Capacity:1063256064 Type:vfs Inodes:524288 HasInodes:true}] DiskMap:map[252:0:{Name:vda Major:252 Minor:0 Size:21474836480 Scheduler:none} 252:16:{Name:vdb Major:252 Minor:16 Size:107374182400 Scheduler:none} 253:0:{Name:dm-0 Major:253 Minor:0 Size:18249416704 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:107369988096 Scheduler:none}] NetworkDevices:[{Name:dummy0 MacAddress:f6:63:26:88:45:3b Speed:0 Mtu:1500} {Name:eth0 MacAddress:52:54:00:4d:af:e5 Speed:0 Mtu:1500} {Name:eth1 MacAddress:52:54:00:fb:9a:86 Speed:0 Mtu:1500} {Name:flannel.1 MacAddress:8e:e5:9e:c7:44:78 Speed:0 Mtu:1450} {Name:kube-ipvs0 MacAddress:9a:02:88:69:1b:4b Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:8370180096 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:3 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:4 Memory:0 Cores:[{Id:0 Threads:[4] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:5 Memory:0 Cores:[{Id:0 Threads:[5] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:6 Memory:0 Cores:[{Id:0 Threads:[6] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:7 Memory:0 Cores:[{Id:0 Threads:[7] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.512426   15437 manager.go:237] Version: {KernelVersion:4.4.184-1.el7.elrepo.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:18.09.7 DockerAPIVersion:1.39 CadvisorVersion: CadvisorRevision:}
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.512729   15437 server.go:629] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.513120   15437 container_manager_linux.go:261] container manager verified user specified cgroup-root exists: []
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.513185   15437 container_manager_linux.go:266] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/data/k8s/k8s/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms}
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.513243   15437 container_manager_linux.go:286] Creating device plugin manager: true
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.513255   15437 manager.go:109] Creating Device Plugin manager at /var/lib/kubelet/device-plugins/kubelet.sock
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.513275   15437 state_mem.go:36] [cpumanager] initializing new in-memory state store
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.518189   15437 state_mem.go:84] [cpumanager] updated default cpuset: ""
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.518209   15437 state_mem.go:92] [cpumanager] updated cpuset assignments: "map[]"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.518227   15437 state_checkpoint.go:100] [cpumanager] state checkpoint: restored state from checkpoint
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.518233   15437 state_checkpoint.go:101] [cpumanager] state checkpoint: defaultCPUSet:
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.518300   15437 server.go:997] Using root directory: /data/k8s/k8s/kubelet
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.518325   15437 kubelet.go:304] Watching apiserver
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.520753   15437 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.520776   15437 client.go:104] Start docker client with request timeout=10m0s
Sep 23 21:49:58 k8stian-m1 kubelet: W0923 21:49:58.522101   15437 docker_service.go:561] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.522129   15437 docker_service.go:238] Hairpin mode set to "hairpin-veth"
Sep 23 21:49:58 k8stian-m1 kubelet: W0923 21:49:58.522282   15437 cni.go:213] Unable to update cni config: No networks found in /etc/cni/net.d
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.536275   15437 docker_service.go:253] Docker cri networking managed by kubernetes.io/no-op
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.543993   15437 docker_service.go:258] Docker Info: &{ID:CVYT:CYF4:NCEX:ASAE:XCTS:DWNC:BC7E:3HF7:FDCV:JNYS:7X3P:BMK6 Containers:21 ContainersRunning:20 ContainersPaused:0 ContainersStopped:1 Images:23 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:true NFd:130 OomKillDisable:true NGoroutines:126 SystemTime:2019-09-23T21:49:58.537674026+08:00 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:4.4.184-1.el7.elrepo.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0006f96c0 NCPU:8 MemTotal:8370180096 GenericResources:[] DockerRootDir:/data/k8s/docker/data HTTPProxy: HTTPSProxy: NoProxy: Name:k8stian-m1 Labels:[] ExperimentalBuild:false ServerVersion:18.09.7 ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:runc Args:[]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil>} LiveRestoreEnabled:true Isolation: InitBinary:docker-init ContainerdCommit:{ID:894b81a4b802e4eb2a91d1ce216b8817763c29fb Expected:894b81a4b802e4eb2a91d1ce216b8817763c29fb} RuncCommit:{ID:425e105d5a03fabd737a126ad93d62a9eeede87f Expected:425e105d5a03fabd737a126ad93d62a9eeede87f} InitCommit:{ID:fec3683 Expected:fec3683} SecurityOptions:[name=seccomp,profile=default]}
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.544073   15437 docker_service.go:271] Setting cgroupDriver to cgroupfs
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.544136   15437 kubelet.go:632] Starting the GRPC server for the docker CRI shim.
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.544157   15437 docker_server.go:59] Start dockershim grpc server
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561134   15437 remote_runtime.go:62] parsed scheme: ""
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561173   15437 remote_runtime.go:62] scheme "" not registered, fallback to default scheme
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561230   15437 remote_image.go:50] parsed scheme: ""
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561237   15437 remote_image.go:50] scheme "" not registered, fallback to default scheme
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561309   15437 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [{/var/run/dockershim.sock 0  <nil>}]
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561327   15437 clientconn.go:796] ClientConn switching balancer to "pick_first"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561379   15437 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc0002c15f0, CONNECTING
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561420   15437 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [{/var/run/dockershim.sock 0  <nil>}]
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561429   15437 clientconn.go:796] ClientConn switching balancer to "pick_first"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561466   15437 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc0002ff950, CONNECTING
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561557   15437 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc0002c15f0, READY
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.561570   15437 balancer_conn_wrappers.go:131] pickfirstBalancer: HandleSubConnStateChange: 0xc0002ff950, READY
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.563175   15437 kuberuntime_manager.go:210] Container runtime docker initialized, version: 18.09.7, apiVersion: 1.39.0
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.563325   15437 kuberuntime_manager.go:950] updating runtime config through cri with podcidr 172.30.0.0/16
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.563630   15437 docker_service.go:353] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:172.30.0.0/16,},}
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.563809   15437 kubelet_network.go:77] Setting Pod CIDR:  -> 172.30.0.0/16
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.563890   15437 certificate_store.go:130] Loading cert/key pair from "/etc/kubernetes/cert/kubelet-server-current.pem".
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575320   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/aws-ebs"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575343   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/empty-dir"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575352   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/gce-pd"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575359   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/git-repo"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575367   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/host-path"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575375   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/nfs"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575382   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/secret"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575390   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/iscsi"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575414   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/glusterfs"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575422   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/rbd"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575428   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/cinder"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575446   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/quobyte"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575455   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/cephfs"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575464   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/downward-api"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575488   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/fc"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575514   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/flocker"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575538   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/azure-file"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575545   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/configmap"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575552   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/vsphere-volume"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575581   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/azure-disk"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575588   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/photon-pd"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575596   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/projected"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575612   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/portworx-volume"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575624   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/scaleio"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575632   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/local-volume"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575639   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/storageos"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.575660   15437 plugins.go:617] Loaded volume plugin "kubernetes.io/csi"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.576758   15437 server.go:1055] Started kubelet
Sep 23 21:49:58 k8stian-m1 kubelet: E0923 21:49:58.576899   15437 kubelet.go:1282] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data in memory cache
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.578064   15437 certificate_manager.go:249] Certificate rotation is enabled.
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.578081   15437 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.578098   15437 status_manager.go:152] Starting to sync pod status with apiserver
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.578112   15437 kubelet.go:1806] Starting kubelet main sync loop.
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.578130   15437 kubelet.go:1823] skipping pod synchronization - [container runtime status check may not have completed yet., PLEG is not healthy: pleg has yet to be successful.]
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.578219   15437 server.go:141] Starting to listen on 10.11.37.71:10250
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.578815   15437 server.go:343] Adding debug handlers to kubelet server.
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.580476   15437 certificate_manager.go:489] Certificate expiration is 2119-06-09 02:16:11 +0000 UTC, rotation deadline is 2103-04-18 07:53:15.316251584 +0000 UTC
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.580501   15437 certificate_manager.go:255] Waiting 732522h3m16.735755937s for next certificate rotation
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.580550   15437 volume_manager.go:246] The desired_state_of_world populator starts
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.580556   15437 volume_manager.go:248] Starting Kubelet Volume Manager
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.583379   15437 desired_state_of_world_populator.go:130] Desired state populator starts to run
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.596822   15437 factory.go:356] Registering Docker factory
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.598120   15437 factory.go:54] Registering systemd factory
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.598267   15437 factory.go:100] Registering Raw factory
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.599364   15437 manager.go:1256] Started watching for new ooms in manager
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.601155   15437 manager.go:369] Starting recovery of all containers
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.678360   15437 kubelet.go:1823] skipping pod synchronization - container runtime status check may not have completed yet.
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.681157   15437 kubelet_node_status.go:283] Setting node annotation to enable volume controller attach/detach
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.683302   15437 kubelet_node_status.go:468] Recording NodeHasSufficientMemory event message for node k8stian-m1
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.683349   15437 kubelet_node_status.go:468] Recording NodeHasNoDiskPressure event message for node k8stian-m1
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.683360   15437 kubelet_node_status.go:468] Recording NodeHasSufficientPID event message for node k8stian-m1
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.683399   15437 kubelet_node_status.go:72] Attempting to register node k8stian-m1
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.697477   15437 kubelet_node_status.go:114] Node k8stian-m1 was previously registered
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.697501   15437 kubelet_node_status.go:75] Successfully registered node k8stian-m1
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.705736   15437 kubelet_node_status.go:468] Recording NodeNotReady event message for node k8stian-m1
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.705765   15437 setters.go:521] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2019-09-23 21:49:58.705715748 +0800 CST m=+3.255727450 LastTransitionTime:2019-09-23 21:49:58.705715748 +0800 CST m=+3.255727450 Reason:KubeletNotReady Message:container runtime status check may not have completed yet.}
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.799182   15437 manager.go:374] Recovery completed
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.867399   15437 cpu_manager.go:155] [cpumanager] starting with none policy
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.867418   15437 cpu_manager.go:156] [cpumanager] reconciling every 10s
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.867426   15437 policy_none.go:42] [cpumanager] none policy: Start
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.868563   15437 manager.go:196] Starting Device Plugin manager
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.868900   15437 container_manager_linux.go:448] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.878515   15437 kubelet.go:1823] skipping pod synchronization - container runtime status check may not have completed yet.
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.886006   15437 manager.go:231] Serving device plugin registration server on "/var/lib/kubelet/device-plugins/kubelet.sock"
Sep 23 21:49:58 k8stian-m1 kubelet: I0923 21:49:58.886071   15437 plugin_watcher.go:90] Plugin Watcher Start at /data/k8s/k8s/kubelet/plugins_registry
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.278823   15437 kubelet.go:1885] SyncLoop (ADD, "api"): "fluentd-es-v2.4.0-j7562_kube-system(6a7d0854-b46b-11e9-b0a6-5254004dafe5), kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5), nfs-client-provisioner-6ff488f674-rwztd_default(859a5584-c7c7-11e9-8012-5254004dafe5), node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5), prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5), speaker-r8zq5_metallb-system(6a7e77dd-b46b-11e9-b0a6-5254004dafe5), monitoring-influxdb-7756cddffb-bwhjk_monitoring(38f22cb2-d076-11e9-8012-5254004dafe5)"
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285010   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "sys" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-sys") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285141   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "root" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-root") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285201   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-state-metrics-token-hd744" (UniqueName: "kubernetes.io/secret/23f67d94-b750-11e9-b0a6-5254004dafe5-kube-state-metrics-token-hd744") pod "kube-state-metrics-7744645f56-brxz4" (UID: "23f67d94-b750-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285244   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "varlibdockercontainers" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlibdockercontainers") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285298   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "proc" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-proc") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285348   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "fluentd-es-token-f982w" (UniqueName: "kubernetes.io/secret/6a7d0854-b46b-11e9-b0a6-5254004dafe5-fluentd-es-token-f982w") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285385   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "node-exporter-token-cn8fv" (UniqueName: "kubernetes.io/secret/6a7820b2-b46b-11e9-b0a6-5254004dafe5-node-exporter-token-cn8fv") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285511   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "speaker-token-8k6ct" (UniqueName: "kubernetes.io/secret/6a7e77dd-b46b-11e9-b0a6-5254004dafe5-speaker-token-8k6ct") pod "speaker-r8zq5" (UID: "6a7e77dd-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285547   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "varlog" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlog") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.285588   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config-volume" (UniqueName: "kubernetes.io/configmap/6a7d0854-b46b-11e9-b0a6-5254004dafe5-config-volume") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.290999   15437 kubelet.go:1930] SyncLoop (PLEG): "nfs-client-provisioner-6ff488f674-rwztd_default(859a5584-c7c7-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"859a5584-c7c7-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"1d6e6dbd6874b041ba405b331fbf91c7e744d43f64c37a0d684c74aa66557d4a"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291052   15437 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"d853767e873ab66aa2db45a3b19e78cbc36b32667e00147298b406a0add3805e"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291086   15437 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"bb0687d84f059cc6b22aec2d57f8a8a004b35ba64e021a6c27f30a2fb299ab12"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291118   15437 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"2c1cea3ecf41edec3d92a52317a2fc12c14cc3a06930be9e4f6d61c4353db2c2"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291148   15437 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"f9c764b9e3241d1b92a54d4e1a71fa2c30b61daa66ce97be14fb04bd0ce37167"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291180   15437 kubelet.go:1930] SyncLoop (PLEG): "kube-state-metrics-7744645f56-brxz4_monitoring(23f67d94-b750-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"23f67d94-b750-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"87a6fef3870cb6aa9d86b7a838bc8e6b162e0fc2f8e1f11db5944f5752824a6e"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291208   15437 kubelet.go:1930] SyncLoop (PLEG): "fluentd-es-v2.4.0-j7562_kube-system(6a7d0854-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7d0854-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"bd6e49c6ae8df802c23712161ffc103d0d0b4aa1fe08dc39b3c1d323db1a875c"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291242   15437 kubelet.go:1930] SyncLoop (PLEG): "fluentd-es-v2.4.0-j7562_kube-system(6a7d0854-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7d0854-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"e54323e007a26abae1bfe6fbd5274ffb3527863e5cf055210be8832ba4a7e7f3"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291270   15437 kubelet.go:1930] SyncLoop (PLEG): "node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7820b2-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"8e47e0857f0e9cec5d66cabcdd3c2ebe1207f53fe80bf32460762c2605704fc0"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291298   15437 kubelet.go:1930] SyncLoop (PLEG): "node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7820b2-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"210a9f040bbc8567c8822d49c7b671085e86d540c1e334fa28ede96309704d42"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291321   15437 kubelet.go:1930] SyncLoop (PLEG): "node-exporter-d46nv_monitoring(6a7820b2-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7820b2-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"5c401a6bfb07210b5be751a49e3a3041da1600e60bc2c7bf5493744c68fcf29a"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291338   15437 kubelet.go:1930] SyncLoop (PLEG): "speaker-r8zq5_metallb-system(6a7e77dd-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7e77dd-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"b80fe3582bc6351d056f60520c702df1354e7fa8ed15410ba0afac396a1e278e"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291353   15437 kubelet.go:1930] SyncLoop (PLEG): "speaker-r8zq5_metallb-system(6a7e77dd-b46b-11e9-b0a6-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"6a7e77dd-b46b-11e9-b0a6-5254004dafe5", Type:"ContainerStarted", Data:"9d6619007c1a8a06f4c9488b9491e5daebae0aef20b2f16a68d75b77edf2f2dc"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291368   15437 kubelet.go:1930] SyncLoop (PLEG): "monitoring-influxdb-7756cddffb-bwhjk_monitoring(38f22cb2-d076-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"38f22cb2-d076-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"ed594b69a5db40142ffd77cc6290eaab557544df08d9956b8bb0d44260e839e1"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291384   15437 kubelet.go:1930] SyncLoop (PLEG): "monitoring-influxdb-7756cddffb-bwhjk_monitoring(38f22cb2-d076-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"38f22cb2-d076-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"38a3a110ec9e7bee26ca708b041680495c3fabb87a80c96157557eec768d6b4a"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291400   15437 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"3e1772ea8c8938b51d8b6568bfca34b4b1253a0f438aa6a28d71be4122cf9f56"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291415   15437 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"825171dca3ada43b511089d384e7f2c8d3eb33fe2db84714ae064109fdad94d4"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291442   15437 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"4821e138ebc9cf4cdf596688074a9dcacb625dce8138a61c194c6fbae873c62d"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291460   15437 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerDied", Data:"e8fb76e27c61cd83762f07b60e9fb928a085ba15a3d5f0e8cdbd8f3a1885ba86"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.291519   15437 kubelet.go:1930] SyncLoop (PLEG): "prometheus-k8s-0_monitoring(98f00373-cf8d-11e9-8012-5254004dafe5)", event: &pleg.PodLifecycleEvent{ID:"98f00373-cf8d-11e9-8012-5254004dafe5", Type:"ContainerStarted", Data:"e4654eda586be529843dfbd5722e116097f4fd857700312f6ce5b5d86220b667"}
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.385883   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "config-volume" (UniqueName: "kubernetes.io/configmap/6a7d0854-b46b-11e9-b0a6-5254004dafe5-config-volume") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.385926   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "fluentd-es-token-f982w" (UniqueName: "kubernetes.io/secret/6a7d0854-b46b-11e9-b0a6-5254004dafe5-fluentd-es-token-f982w") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.385949   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "node-exporter-token-cn8fv" (UniqueName: "kubernetes.io/secret/6a7820b2-b46b-11e9-b0a6-5254004dafe5-node-exporter-token-cn8fv") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.385980   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "speaker-token-8k6ct" (UniqueName: "kubernetes.io/secret/6a7e77dd-b46b-11e9-b0a6-5254004dafe5-speaker-token-8k6ct") pod "speaker-r8zq5" (UID: "6a7e77dd-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.386001   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "varlog" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlog") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.386040   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "proc" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-proc") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.386081   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "varlog" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlog") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399042   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "proc" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-proc") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399107   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "node-exporter-token-cn8fv" (UniqueName: "kubernetes.io/secret/6a7820b2-b46b-11e9-b0a6-5254004dafe5-node-exporter-token-cn8fv") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399315   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "fluentd-es-token-f982w" (UniqueName: "kubernetes.io/secret/6a7d0854-b46b-11e9-b0a6-5254004dafe5-fluentd-es-token-f982w") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399373   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "speaker-token-8k6ct" (UniqueName: "kubernetes.io/secret/6a7e77dd-b46b-11e9-b0a6-5254004dafe5-speaker-token-8k6ct") pod "speaker-r8zq5" (UID: "6a7e77dd-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399501   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "sys" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-sys") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399528   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "root" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-root") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399552   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "kube-state-metrics-token-hd744" (UniqueName: "kubernetes.io/secret/23f67d94-b750-11e9-b0a6-5254004dafe5-kube-state-metrics-token-hd744") pod "kube-state-metrics-7744645f56-brxz4" (UID: "23f67d94-b750-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399583   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "varlibdockercontainers" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlibdockercontainers") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399595   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "config-volume" (UniqueName: "kubernetes.io/configmap/6a7d0854-b46b-11e9-b0a6-5254004dafe5-config-volume") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399630   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "sys" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-sys") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399701   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "varlibdockercontainers" (UniqueName: "kubernetes.io/host-path/6a7d0854-b46b-11e9-b0a6-5254004dafe5-varlibdockercontainers") pod "fluentd-es-v2.4.0-j7562" (UID: "6a7d0854-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399807   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "root" (UniqueName: "kubernetes.io/host-path/6a7820b2-b46b-11e9-b0a6-5254004dafe5-root") pod "node-exporter-d46nv" (UID: "6a7820b2-b46b-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.399907   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "kube-state-metrics-token-hd744" (UniqueName: "kubernetes.io/secret/23f67d94-b750-11e9-b0a6-5254004dafe5-kube-state-metrics-token-hd744") pod "kube-state-metrics-7744645f56-brxz4" (UID: "23f67d94-b750-11e9-b0a6-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500420   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "secret-controller-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-controller-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500532   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "secret-etcd-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-etcd-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500616   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-0a3b5547-cff5-11e9-931b-52540084153b" (UniqueName: "kubernetes.io/nfs/38f22cb2-d076-11e9-8012-5254004dafe5-pvc-0a3b5547-cff5-11e9-931b-52540084153b") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500673   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-config") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500717   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-526xt" (UniqueName: "kubernetes.io/secret/38f22cb2-d076-11e9-8012-5254004dafe5-default-token-526xt") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500753   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "nfs-client-root" (UniqueName: "kubernetes.io/nfs/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-root") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500795   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "nfs-client-provisioner-token-6kd7d" (UniqueName: "kubernetes.io/secret/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-provisioner-token-6kd7d") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500819   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvc-98ef3446-cf8d-11e9-8012-5254004dafe5" (UniqueName: "kubernetes.io/nfs/98f00373-cf8d-11e9-8012-5254004dafe5-pvc-98ef3446-cf8d-11e9-8012-5254004dafe5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500839   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/98f00373-cf8d-11e9-8012-5254004dafe5-config-out") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500860   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500883   15437 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-k8s-token-ncfk5" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-token-ncfk5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.500894   15437 reconciler.go:154] Reconciler: start to sync state
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757614   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "secret-etcd-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-etcd-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757676   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "pvc-0a3b5547-cff5-11e9-931b-52540084153b" (UniqueName: "kubernetes.io/nfs/38f22cb2-d076-11e9-8012-5254004dafe5-pvc-0a3b5547-cff5-11e9-931b-52540084153b") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757703   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "config" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-config") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757727   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "default-token-526xt" (UniqueName: "kubernetes.io/secret/38f22cb2-d076-11e9-8012-5254004dafe5-default-token-526xt") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757765   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "nfs-client-root" (UniqueName: "kubernetes.io/nfs/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-root") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757789   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "nfs-client-provisioner-token-6kd7d" (UniqueName: "kubernetes.io/secret/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-provisioner-token-6kd7d") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757812   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "pvc-98ef3446-cf8d-11e9-8012-5254004dafe5" (UniqueName: "kubernetes.io/nfs/98f00373-cf8d-11e9-8012-5254004dafe5-pvc-98ef3446-cf8d-11e9-8012-5254004dafe5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757848   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/98f00373-cf8d-11e9-8012-5254004dafe5-config-out") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757869   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757889   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "prometheus-k8s-token-ncfk5" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-token-ncfk5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.757909   15437 reconciler.go:252] operationExecutor.MountVolume started for volume "secret-controller-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-controller-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.758309   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "secret-etcd-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-etcd-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.758390   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "pvc-0a3b5547-cff5-11e9-931b-52540084153b" (UniqueName: "kubernetes.io/nfs/38f22cb2-d076-11e9-8012-5254004dafe5-pvc-0a3b5547-cff5-11e9-931b-52540084153b") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.758674   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "config" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-config") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.758864   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "default-token-526xt" (UniqueName: "kubernetes.io/secret/38f22cb2-d076-11e9-8012-5254004dafe5-default-token-526xt") pod "monitoring-influxdb-7756cddffb-bwhjk" (UID: "38f22cb2-d076-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.759214   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "secret-controller-certs" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-secret-controller-certs") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.759385   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "nfs-client-root" (UniqueName: "kubernetes.io/nfs/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-root") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.759391   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "nfs-client-provisioner-token-6kd7d" (UniqueName: "kubernetes.io/secret/859a5584-c7c7-11e9-8012-5254004dafe5-nfs-client-provisioner-token-6kd7d") pod "nfs-client-provisioner-6ff488f674-rwztd" (UID: "859a5584-c7c7-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.759947   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "prometheus-k8s-token-ncfk5" (UniqueName: "kubernetes.io/secret/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-token-ncfk5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.759947   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "prometheus-k8s-rulefiles-0" (UniqueName: "kubernetes.io/configmap/98f00373-cf8d-11e9-8012-5254004dafe5-prometheus-k8s-rulefiles-0") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.760033   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "pvc-98ef3446-cf8d-11e9-8012-5254004dafe5" (UniqueName: "kubernetes.io/nfs/98f00373-cf8d-11e9-8012-5254004dafe5-pvc-98ef3446-cf8d-11e9-8012-5254004dafe5") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:49:59 k8stian-m1 kubelet: I0923 21:49:59.765279   15437 operation_generator.go:669] MountVolume.SetUp succeeded for volume "config-out" (UniqueName: "kubernetes.io/empty-dir/98f00373-cf8d-11e9-8012-5254004dafe5-config-out") pod "prometheus-k8s-0" (UID: "98f00373-cf8d-11e9-8012-5254004dafe5")
Sep 23 21:50:01 k8stian-m1 systemd: Started Session 14185 of user root.
Sep 23 21:50:08 k8stian-m1 kubelet: I0923 21:50:08.721157   15437 kubelet_node_status.go:468] Recording NodeReady event message for node k8stian-m1
Sep 23 21:51:47 k8stian-m1 systemd: Started Kubernetes Kube-Proxy Server.
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298586   16033 flags.go:33] FLAG: --alsologtostderr="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298645   16033 flags.go:33] FLAG: --bind-address="0.0.0.0"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298653   16033 flags.go:33] FLAG: --cleanup="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298658   16033 flags.go:33] FLAG: --cleanup-iptables="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298669   16033 flags.go:33] FLAG: --cleanup-ipvs="true"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298672   16033 flags.go:33] FLAG: --cluster-cidr=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298676   16033 flags.go:33] FLAG: --config="/etc/kubernetes/kube-proxy-config.yaml"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298680   16033 flags.go:33] FLAG: --config-sync-period="15m0s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298684   16033 flags.go:33] FLAG: --conntrack-max="0"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298688   16033 flags.go:33] FLAG: --conntrack-max-per-core="32768"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298691   16033 flags.go:33] FLAG: --conntrack-min="131072"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298694   16033 flags.go:33] FLAG: --conntrack-tcp-timeout-close-wait="1h0m0s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298697   16033 flags.go:33] FLAG: --conntrack-tcp-timeout-established="24h0m0s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298700   16033 flags.go:33] FLAG: --feature-gates=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298705   16033 flags.go:33] FLAG: --healthz-bind-address="0.0.0.0:10256"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298708   16033 flags.go:33] FLAG: --healthz-port="10256"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298711   16033 flags.go:33] FLAG: --help="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298714   16033 flags.go:33] FLAG: --hostname-override=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298717   16033 flags.go:33] FLAG: --iptables-masquerade-bit="14"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298720   16033 flags.go:33] FLAG: --iptables-min-sync-period="0s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298722   16033 flags.go:33] FLAG: --iptables-sync-period="30s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298725   16033 flags.go:33] FLAG: --ipvs-exclude-cidrs="[]"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298730   16033 flags.go:33] FLAG: --ipvs-min-sync-period="0s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298733   16033 flags.go:33] FLAG: --ipvs-scheduler=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298735   16033 flags.go:33] FLAG: --ipvs-strict-arp="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298738   16033 flags.go:33] FLAG: --ipvs-sync-period="30s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298741   16033 flags.go:33] FLAG: --kube-api-burst="10"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298753   16033 flags.go:33] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298759   16033 flags.go:33] FLAG: --kube-api-qps="5"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298763   16033 flags.go:33] FLAG: --kubeconfig=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298766   16033 flags.go:33] FLAG: --log-backtrace-at=":0"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298788   16033 flags.go:33] FLAG: --log-dir=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298792   16033 flags.go:33] FLAG: --log-file=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298795   16033 flags.go:33] FLAG: --log-flush-frequency="5s"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298798   16033 flags.go:33] FLAG: --logtostderr="true"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298801   16033 flags.go:33] FLAG: --masquerade-all="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298805   16033 flags.go:33] FLAG: --master=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298808   16033 flags.go:33] FLAG: --metrics-bind-address="127.0.0.1:10249"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298817   16033 flags.go:33] FLAG: --metrics-port="10249"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298820   16033 flags.go:33] FLAG: --nodeport-addresses="[]"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298825   16033 flags.go:33] FLAG: --oom-score-adj="-999"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298828   16033 flags.go:33] FLAG: --profiling="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298831   16033 flags.go:33] FLAG: --proxy-mode=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298835   16033 flags.go:33] FLAG: --proxy-port-range=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298839   16033 flags.go:33] FLAG: --resource-container="/kube-proxy"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298842   16033 flags.go:33] FLAG: --skip-headers="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298845   16033 flags.go:33] FLAG: --stderrthreshold="2"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298848   16033 flags.go:33] FLAG: --udp-timeout="250ms"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298851   16033 flags.go:33] FLAG: --v="2"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298854   16033 flags.go:33] FLAG: --version="false"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298859   16033 flags.go:33] FLAG: --vmodule=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.298862   16033 flags.go:33] FLAG: --write-config-to=""
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.303936   16033 feature_gate.go:226] feature gates: &{map[]}
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.460204   16033 server_others.go:176] Using ipvs Proxier.
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.460374   16033 proxier.go:377] nodeIP: 10.11.37.71, isIPv6: false
Sep 23 21:51:47 k8stian-m1 kube-proxy: W0923 21:51:47.460408   16033 proxier.go:386] IPVS scheduler not specified, use rr by default
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.463366   16033 server.go:562] Version: v1.14.5
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.477511   16033 server.go:578] Running in resource-only container "/kube-proxy"
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.478251   16033 conntrack.go:52] Setting nf_conntrack_max to 262144
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.478487   16033 config.go:102] Starting endpoints config controller
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.478505   16033 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.478558   16033 config.go:202] Starting service config controller
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.478576   16033 controller_utils.go:1027] Waiting for caches to sync for service config controller
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578752   16033 controller_utils.go:1034] Caches are synced for endpoints config controller
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578816   16033 controller_utils.go:1034] Caches are synced for service config controller
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578844   16033 proxier.go:733] Not syncing ipvs rules until Services and Endpoints have been received from master
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578916   16033 service.go:332] Adding new service port "kube-system/kibana-logging:" at 172.31.102.162:5601/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578937   16033 service.go:332] Adding new service port "default/nginx3:http" at 172.31.178.36:80/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578947   16033 service.go:332] Adding new service port "default/kubernetes:https" at 172.31.0.1:443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578956   16033 service.go:332] Adding new service port "kube-system/elasticsearch-logging:" at 172.31.175.58:9200/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578964   16033 service.go:332] Adding new service port "kube-system/metrics-server:" at 172.31.214.155:443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578973   16033 service.go:332] Adding new service port "istio-system/istio-citadel:grpc-citadel" at 172.31.187.148:8060/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578980   16033 service.go:332] Adding new service port "istio-system/istio-citadel:http-monitoring" at 172.31.187.148:15014/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578989   16033 service.go:332] Adding new service port "monitoring/prometheus-adapter:https" at 172.31.27.117:443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.578997   16033 service.go:332] Adding new service port "istio-system/istio-telemetry:grpc-mixer" at 172.31.179.130:9091/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579011   16033 service.go:332] Adding new service port "istio-system/istio-telemetry:grpc-mixer-mtls" at 172.31.179.130:15004/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579019   16033 service.go:332] Adding new service port "istio-system/istio-telemetry:http-monitoring" at 172.31.179.130:15014/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579027   16033 service.go:332] Adding new service port "istio-system/istio-telemetry:prometheus" at 172.31.179.130:42422/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579036   16033 service.go:332] Adding new service port "kube-system/kube-dns:dns-tcp" at 172.31.0.2:53/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579044   16033 service.go:332] Adding new service port "kube-system/kube-dns:metrics" at 172.31.0.2:9153/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579055   16033 service.go:332] Adding new service port "kube-system/kube-dns:dns" at 172.31.0.2:53/UDP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579063   16033 service.go:332] Adding new service port "istio-system/istio-policy:grpc-mixer" at 172.31.234.5:9091/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579071   16033 service.go:332] Adding new service port "istio-system/istio-policy:grpc-mixer-mtls" at 172.31.234.5:15004/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579079   16033 service.go:332] Adding new service port "istio-system/istio-policy:http-monitoring" at 172.31.234.5:15014/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579087   16033 service.go:332] Adding new service port "kube-system/kubernetes-dashboard:" at 172.31.141.21:443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579096   16033 service.go:332] Adding new service port "kube-system/tiller-deploy:tiller" at 172.31.179.216:44134/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579105   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:tcp" at 172.31.124.201:31400/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579113   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:https-kiali" at 172.31.124.201:15029/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579120   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:https-tracing" at 172.31.124.201:15032/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579128   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:status-port" at 172.31.124.201:15020/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579138   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:http2" at 172.31.124.201:80/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579146   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:https" at 172.31.124.201:443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579154   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:https-prometheus" at 172.31.124.201:15030/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579165   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:https-grafana" at 172.31.124.201:15031/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579178   16033 service.go:332] Adding new service port "istio-system/istio-ingressgateway:tls" at 172.31.124.201:15443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579188   16033 service.go:332] Adding new service port "monitoring/grafana:http" at 172.31.54.32:3000/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579196   16033 service.go:332] Adding new service port "monitoring/monitoring-influxdb:" at 172.31.8.35:8086/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579203   16033 service.go:332] Adding new service port "istio-system/istio-sidecar-injector:" at 172.31.1.11:443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579211   16033 service.go:332] Adding new service port "istio-system/prometheus:http-prometheus" at 172.31.112.113:9090/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579220   16033 service.go:332] Adding new service port "default/website:http" at 172.31.231.164:80/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579228   16033 service.go:332] Adding new service port "monitoring/prometheus-k8s:web" at 172.31.138.136:9090/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579238   16033 service.go:332] Adding new service port "default/myapp:http" at 172.31.68.205:80/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579246   16033 service.go:332] Adding new service port "default/sample-webapp:" at 172.31.199.136:8080/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579254   16033 service.go:332] Adding new service port "monitoring/alertmanager-main:web" at 172.31.45.210:9093/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579262   16033 service.go:332] Adding new service port "istio-system/istio-galley:https-validation" at 172.31.17.10:443/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579269   16033 service.go:332] Adding new service port "istio-system/istio-galley:http-monitoring" at 172.31.17.10:15014/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579277   16033 service.go:332] Adding new service port "istio-system/istio-galley:grpc-mcp" at 172.31.17.10:9901/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579285   16033 service.go:332] Adding new service port "istio-system/istio-pilot:grpc-xds" at 172.31.118.86:15010/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579292   16033 service.go:332] Adding new service port "istio-system/istio-pilot:https-xds" at 172.31.118.86:15011/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579300   16033 service.go:332] Adding new service port "istio-system/istio-pilot:http-legacy-discovery" at 172.31.118.86:8080/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579310   16033 service.go:332] Adding new service port "istio-system/istio-pilot:http-monitoring" at 172.31.118.86:15014/TCP
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.579459   16033 proxier.go:747] Stale udp service kube-system/kube-dns:dns -> 172.31.0.2
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.721326   16033 proxier.go:1791] Opened local port "nodePort for default/nginx3:http" (:32147/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.724756   16033 graceful_termination.go:161] Trying to delete rs: 172.31.17.10:9901/TCP/172.30.56.14:9901
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.724798   16033 graceful_termination.go:175] Deleting rs: 172.31.17.10:9901/TCP/172.30.56.14:9901
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.725096   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:status-port" (:32497/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.726034   16033 proxier.go:1791] Opened local port "nodePort for kube-system/kubernetes-dashboard:" (:31660/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.726708   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:http2" (:31380/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.727357   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:https" (:31390/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.728444   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:https-kiali" (:32435/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.730258   16033 graceful_termination.go:161] Trying to delete rs: 172.31.17.10:443/TCP/172.30.56.14:443
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.730291   16033 graceful_termination.go:175] Deleting rs: 172.31.17.10:443/TCP/172.30.56.14:443
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.730532   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:tcp" (:31400/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.732140   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:https-tracing" (:32577/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.732924   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:tls" (:30292/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.734263   16033 graceful_termination.go:161] Trying to delete rs: 172.31.17.10:15014/TCP/172.30.56.14:15014
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.734329   16033 graceful_termination.go:175] Deleting rs: 172.31.17.10:15014/TCP/172.30.56.14:15014
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.734433   16033 graceful_termination.go:161] Trying to delete rs: 172.31.0.1:443/TCP/10.11.37.71:6443
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.734455   16033 graceful_termination.go:175] Deleting rs: 172.31.0.1:443/TCP/10.11.37.71:6443
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.734645   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:https-prometheus" (:30586/tcp)
Sep 23 21:51:47 k8stian-m1 kube-proxy: I0923 21:51:47.735235   16033 proxier.go:1791] Opened local port "nodePort for istio-system/istio-ingressgateway:https-grafana" (:30267/tcp)
